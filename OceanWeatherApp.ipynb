{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hollimey/capstone-application/blob/main/OceanWeatherApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebSzmQPSDjS9"
      },
      "source": [
        "### Get User Location Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a78db619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb21494b-e7a9-43b0-bec7-e8f9f2ad3549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a U.S. location to get weather information (ex. Seattle, WA): seattle, wa\n"
          ]
        }
      ],
      "source": [
        "# get user location input (U.S. only)\n",
        "user_location = input(\"Enter a U.S. location to get weather information (ex. Seattle, WA): \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3D_nNAcDU1i"
      },
      "source": [
        "#### Convert to Coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "3894bff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab82ecbe-0a6e-4ae7-ee7b-5dd988440775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Coordinates: 47.6038321, -122.330062\n"
          ]
        }
      ],
      "source": [
        "# use geocoding library to convert city name into lat/lon coordinates\n",
        "\n",
        "%pip install geopy\n",
        "\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"weather_app\")\n",
        "\n",
        "try:\n",
        "    user_location_geocoded = geolocator.geocode(user_location)\n",
        "    if user_location_geocoded:\n",
        "        latitude = user_location_geocoded.latitude\n",
        "        longitude = user_location_geocoded.longitude\n",
        "        print(f\"Coordinates: {latitude}, {longitude}\")\n",
        "    else:\n",
        "        print(f\"Location '{user_location}' not found.\")\n",
        "        latitude = None\n",
        "        longitude = None\n",
        "except (GeocoderTimedOut, GeocoderUnavailable) as e:\n",
        "    print(f\"Geocoding service error: {e}\")\n",
        "    latitude = None\n",
        "    longitude = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxnXNpLJ7YR"
      },
      "source": [
        "### NOAA Weather Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "5abf888e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ff3153-a661-43e1-da90-5e9483d8e07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined state for alerts: WA\n",
            "Parsed 5 active weather alerts.\n",
            "\n",
            "Atmospheric data and alerts saved to user_data_noaa_atmos.json\n"
          ]
        }
      ],
      "source": [
        "# fetch NOAA weather data from API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_weather_data(latitude, longitude):\n",
        "    point_url = f\"https://api.weather.gov/points/{latitude},{longitude}\"\n",
        "    try:\n",
        "        response = requests.get(point_url)\n",
        "        response.raise_for_status()  # http error for bad responses\n",
        "        point_data = response.json()\n",
        "\n",
        "        forecast_url = point_data['properties']['forecast']\n",
        "        hourly_forecast_url = point_data['properties']['forecastHourly']\n",
        "        gridpoints_url = f\"https://api.weather.gov/gridpoints/{point_data['properties']['gridId']}/{point_data['properties']['gridX']},{point_data['properties']['gridY']}/forecast\"\n",
        "\n",
        "        # fetch data\n",
        "        forecast_response = requests.get(forecast_url)\n",
        "        forecast_response.raise_for_status()\n",
        "        forecast_data = forecast_response.json()\n",
        "        hourly_forecast_response = requests.get(hourly_forecast_url)\n",
        "        hourly_forecast_response.raise_for_status()\n",
        "        hourly_forecast_data = hourly_forecast_response.json()\n",
        "        gridpoints_response = requests.get(gridpoints_url)\n",
        "        gridpoints_response.raise_for_status()\n",
        "        gridpoints_data = gridpoints_response.json()\n",
        "\n",
        "        # return dictionary\n",
        "        return {\n",
        "            \"point_data\": point_data,\n",
        "            \"forecast\": forecast_data,\n",
        "            \"hourly_forecast\": hourly_forecast_data,\n",
        "            \"gridpoints\": gridpoints_data\n",
        "        }\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching atmospheric data: {e}\")\n",
        "        return None\n",
        "\n",
        "weather_data_result = fetch_weather_data(latitude, longitude)\n",
        "\n",
        "# determine the state to fetch alerts\n",
        "state_code = None\n",
        "if weather_data_result and 'point_data' in weather_data_result and \\\n",
        "   'properties' in weather_data_result['point_data'] and \\\n",
        "   'relativeLocation' in weather_data_result['point_data']['properties'] and \\\n",
        "   'properties' in weather_data_result['point_data']['properties']['relativeLocation'] and \\\n",
        "   'state' in weather_data_result['point_data']['properties']['relativeLocation']['properties']:\n",
        "    state_code = weather_data_result['point_data']['properties']['relativeLocation']['properties']['state']\n",
        "    print(f\"Determined state for alerts: {state_code}\")\n",
        "else:\n",
        "    print(\"Could not determine state for fetching alerts.\")\n",
        "\n",
        "# initialize\n",
        "parsed_alerts = []\n",
        "alerts_url = None\n",
        "\n",
        "# fetch and parse alerts if state_code is available\n",
        "if state_code:\n",
        "    alerts_url = f\"https://api.weather.gov/alerts/active?area={state_code}\"\n",
        "\n",
        "    try:\n",
        "        alerts_response = requests.get(alerts_url)\n",
        "        if alerts_response.status_code == 200:\n",
        "\n",
        "            # parse json response\n",
        "            weather_alerts_data = alerts_response.json()\n",
        "            if weather_alerts_data and 'features' in weather_alerts_data:\n",
        "                for feature in weather_alerts_data['features']:\n",
        "                    if 'properties' in feature:\n",
        "                        properties = feature['properties']\n",
        "                        alert_info = {\n",
        "                            'event': properties.get('event'),\n",
        "                            'headline': properties.get('headline'),\n",
        "                            'description': properties.get('description'),\n",
        "                            'effective': properties.get('effective'),\n",
        "                            'expires': properties.get('expires')\n",
        "                        }\n",
        "                        parsed_alerts.append(alert_info)\n",
        "            print(f\"Parsed {len(parsed_alerts)} active weather alerts.\")\n",
        "        else:\n",
        "            print(f\"Error fetching weather alerts: Status code {alerts_response.status_code}\")\n",
        "            print(f\"Response content:\\n{alerts_response.text}\")\n",
        "            weather_alerts_data = None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during weather alerts request: {e}\")\n",
        "        weather_alerts_data = None\n",
        "else:\n",
        "    print(\"State code/abbreviation not available. Cannot fetch alerts.\")\n",
        "    weather_alerts_data = None\n",
        "\n",
        "# save to json file\n",
        "if weather_data_result:\n",
        "    # remove useless \"@context\" key information:\n",
        "    # from point_data\n",
        "    if 'point_data' in weather_data_result and '@context' in weather_data_result['point_data']:\n",
        "        del weather_data_result['point_data']['@context']\n",
        "    # from forecast\n",
        "    if 'forecast' in weather_data_result and '@context' in weather_data_result['forecast']:\n",
        "        del weather_data_result['forecast']['@context']\n",
        "    # from hourly_forecast\n",
        "    if 'hourly_forecast' in weather_data_result and '@context' in weather_data_result['hourly_forecast']:\n",
        "        del weather_data_result['hourly_forecast']['@context']\n",
        "    # from gridpoints\n",
        "    if 'gridpoints' in weather_data_result and '@context' in weather_data_result['gridpoints']:\n",
        "        del weather_data_result['gridpoints']['@context']\n",
        "\n",
        "    # remove empty \"detailedForecast\" information:\n",
        "    # from hourly_forecast periods\n",
        "    if 'hourly_forecast' in weather_data_result and 'properties' in weather_data_result['hourly_forecast'] and 'periods' in weather_data_result['hourly_forecast']['properties']:\n",
        "        for period in weather_data_result['hourly_forecast']['properties']['periods']:\n",
        "            if 'detailedForecast' in period:\n",
        "                del period['detailedForecast']\n",
        "\n",
        "    # remove empty \"name\" information:\n",
        "    # from hourly_forecast periods\n",
        "    if 'hourly_forecast' in weather_data_result and 'properties' in weather_data_result['hourly_forecast'] and 'periods' in weather_data_result['hourly_forecast']['properties']:\n",
        "        for period in weather_data_result['hourly_forecast']['properties']['periods']:\n",
        "            if 'name' in period:\n",
        "                del period['name']\n",
        "\n",
        "    # remove useless \"icon\" information:\n",
        "    # from hourly_forecast periods\n",
        "    if 'hourly_forecast' in weather_data_result and 'properties' in weather_data_result['hourly_forecast'] and 'periods' in weather_data_result['hourly_forecast']['properties']:\n",
        "        for period in weather_data_result['hourly_forecast']['properties']['periods']:\n",
        "            if 'icon' in period:\n",
        "                del period['icon']\n",
        "\n",
        "    # remove empty \"temperatureTrend\" information:\n",
        "    # from gridpoints periods\n",
        "    if 'gridpoints' in weather_data_result and 'properties' in weather_data_result['gridpoints'] and 'periods' in weather_data_result['gridpoints']['properties']:\n",
        "        for period in weather_data_result['gridpoints']['properties']['periods']:\n",
        "            if 'temperatureTrend' in period:\n",
        "                del period['temperatureTrend']\n",
        "    # from hourly_forecast periods\n",
        "    if 'hourly_forecast' in weather_data_result and 'properties' in weather_data_result['hourly_forecast'] and 'periods' in weather_data_result['hourly_forecast']['properties']:\n",
        "        for period in weather_data_result['hourly_forecast']['properties']['periods']:\n",
        "            if 'temperatureTrend' in period:\n",
        "                del period['temperatureTrend']\n",
        "    # from forecast periods\n",
        "    if 'forecast' in weather_data_result and 'properties' in weather_data_result['forecast'] and 'periods' in weather_data_result['forecast']['properties']:\n",
        "        for period in weather_data_result['forecast']['properties']['periods']:\n",
        "            if 'temperatureTrend' in period:\n",
        "                del period['temperatureTrend']\n",
        "\n",
        "    # structure json file\n",
        "    weather_output = {\n",
        "        \"user_location\": {\n",
        "            \"latitude\": latitude,\n",
        "            \"longitude\": longitude,\n",
        "            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "        },\n",
        "        \"atmospheric_data\": weather_data_result,\n",
        "        \"weather_alerts\": parsed_alerts,\n",
        "        \"alerts_source_url\": alerts_url\n",
        "    }\n",
        "    weather_file = \"user_data_noaa_atmos.json\"\n",
        "    try:\n",
        "        with open(weather_file, 'w') as f:\n",
        "            json.dump(weather_output, f, indent=4)\n",
        "        print(f\"\\nAtmospheric data and alerts saved to {weather_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving atmospheric data and alerts to JSON: {e}\")\n",
        "else:\n",
        "    print(\"Atmospheric data has not been fetched yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02f8ec9"
      },
      "source": [
        "### EPA Air Quality Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "8e44f659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc941f63-e164-434f-a2db-ec78075ea56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Air quality data saved to user_data_epa_aq.json\n"
          ]
        }
      ],
      "source": [
        "# fetch AQ data from EPA API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# note: consider adding a secure method for handling API keys later\n",
        "airnow_api_key = \"17C7530F-6ED9-40D5-8DA3-A8328CB8F1B0\"\n",
        "airnow_url = f\"https://www.airnowapi.org/aq/observation/latLong/current/?format=application/json&latitude={latitude}&longitude={longitude}&distance=25&API_KEY={airnow_api_key}\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(airnow_url)\n",
        "    response.raise_for_status() # raise http error for bad responses\n",
        "    aq_data = response.json()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching air quality data: {e}\")\n",
        "    aq_data = None\n",
        "\n",
        "# save to json file\n",
        "if aq_data:\n",
        "    aq_file = \"user_data_epa_aq.json\"\n",
        "\n",
        "    output_data = {\n",
        "        \"user_location\": {\n",
        "            \"latitude\": latitude,\n",
        "            \"longitude\": longitude,\n",
        "            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "        },\n",
        "        \"air_quality_data\": aq_data # include fetched AQ data under a new key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # write to json file\n",
        "        with open(aq_file, 'w') as f:\n",
        "            json.dump(output_data, f, indent=4)\n",
        "        print(f\"Air quality data saved to {aq_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving air quality data to JSON: {e}\")\n",
        "else:\n",
        "    print(\"Air quality data not available to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMs9YB-Nh2o1"
      },
      "source": [
        "### NOAA Buoy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "7186fde0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05497472-8b1c-4880-9072-5c360fa6aee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buoy data saved to dataset_noaa_buoy.json\n"
          ]
        }
      ],
      "source": [
        "# fetch and parse ocean buoy data\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import json\n",
        "\n",
        "buoy_url = \"https://www.ndbc.noaa.gov/data/latest_obs/latest_obs.txt\"\n",
        "\n",
        "try:\n",
        "  response = requests.get(buoy_url)\n",
        "  response.raise_for_status()\n",
        "  text_content = response.text\n",
        "\n",
        "  # parse the text content into a pandas df\n",
        "  data_lines = text_content.strip().split('\\n')\n",
        "  header_line_index = -1\n",
        "  for i, line in enumerate(data_lines):\n",
        "      if line.startswith('#'):\n",
        "          header_line_index = i\n",
        "          break\n",
        "\n",
        "  if header_line_index != -1:\n",
        "      buoy_dataframe = pd.read_csv(StringIO('\\n'.join(data_lines[header_line_index:])), sep='\\s+', skiprows=[1]) # skips units row\n",
        "\n",
        "      # save to json file\n",
        "      ocean_buoy_file = \"dataset_noaa_buoy.json\"\n",
        "      try:\n",
        "          # convert df to a list of dictionaries for json serialization\n",
        "          buoy_data_list = buoy_dataframe.to_dict(orient='records')\n",
        "\n",
        "          # structure json file\n",
        "          output_data = {\n",
        "              \"buoy_observations\": buoy_data_list\n",
        "          }\n",
        "\n",
        "          with open(ocean_buoy_file, 'w') as f:\n",
        "              json.dump(output_data, f, indent=4)\n",
        "          print(f\"Buoy data saved to {ocean_buoy_file}\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving NOAA buoy data to JSON: {e}\")\n",
        "  else:\n",
        "      print(\"Could not find header line in NOAA buoy data.\")\n",
        "      buoy_dataframe = None\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching NOAA buoy data: {e}\")\n",
        "    buoy_dataframe = None\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing NOAA buoy data: {e}\")\n",
        "    buoy_dataframe = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa58acc9"
      },
      "source": [
        "#### Nearest Stations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "0d0c897a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0483daa-5954-461a-8333-d89143bfc3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found data for 12 out of 14 parameters within the 80-mile radius.\n",
            "\n",
            "Nearest buoy data saved to user_data_noaa_buoy.json\n"
          ]
        }
      ],
      "source": [
        "from geopy.distance import geodesic\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "if latitude is None or longitude is None:\n",
        "    print(\"User location could not be determined.\")\n",
        "    buoy_dataframe = None\n",
        "\n",
        "# calculate distance between user and each buoy\n",
        "if buoy_dataframe is not None:\n",
        "    def calculate_distance(row):\n",
        "        station_lat = pd.to_numeric(row['LAT'], errors='coerce')\n",
        "        station_lon = pd.to_numeric(row['LON'], errors='coerce')\n",
        "\n",
        "        if pd.isna(station_lat) or pd.isna(station_lon):\n",
        "            return float('inf')\n",
        "        try:\n",
        "            user_coords = (latitude, longitude)\n",
        "            station_coords = (station_lat, station_lon)\n",
        "            return geodesic(user_coords, station_coords).miles\n",
        "        except ValueError:\n",
        "            return float('inf')\n",
        "    buoy_dataframe['Distance'] = buoy_dataframe.apply(calculate_distance, axis=1)\n",
        "    nearest_stations_df = buoy_dataframe.sort_values(by='Distance').reset_index(drop=True) # sort by distance\n",
        "\n",
        "    requested_parameters = {\n",
        "        'wind_direction': 'WDIR',\n",
        "        'wind_speed': 'WSPD',\n",
        "        'wind_gust': 'GST',\n",
        "        'wave_height': 'WVHT',\n",
        "        'dominant_wave_period': 'DPD',\n",
        "        'avg_wave_period': 'APD',\n",
        "        'mean_wave_direction': 'MWD',\n",
        "        'atmos_pressure': 'PRES',\n",
        "        'pressure_tendency': 'PTDY',\n",
        "        'air_temp': 'ATMP',\n",
        "        'water_temp': 'WTMP',\n",
        "        'dewpoint_temp': 'DEWP',\n",
        "        'visibility': 'VIS',\n",
        "        'tide': 'TIDE'\n",
        "    }\n",
        "\n",
        "    extracted_data = {}\n",
        "    parameters_found = {\n",
        "        param: False for param in requested_parameters.keys()\n",
        "        }\n",
        "    searched_stations = []\n",
        "    search_radius = 80 # define the search radius\n",
        "\n",
        "    # iterate through stations starting from the nearest\n",
        "    for index, station in nearest_stations_df.iterrows():\n",
        "        station_id = station['#STN']\n",
        "        station_distance = round(station['Distance'], 2) # round distance\n",
        "\n",
        "        # check if the station is within radius\n",
        "        if station_distance <= search_radius:\n",
        "            searched_stations.append({'id': station_id, 'distance': station_distance})\n",
        "            all_found_for_this_station = True\n",
        "\n",
        "            for param_name, col_name in requested_parameters.items():\n",
        "                if not parameters_found[param_name]:\n",
        "                    param_value = station.get(col_name)\n",
        "\n",
        "                    if pd.notna(param_value) and str(param_value).strip().upper() != 'MM':\n",
        "                        measurement_unit = parameter_metadata.get(col_name, \"\").split('(')[-1].replace(')', '')\n",
        "\n",
        "                        try:\n",
        "                            year = int(station.get('YYYY'))\n",
        "                            month = int(station.get('MM'))\n",
        "                            day = int(station.get('DD'))\n",
        "                            hour = int(station.get('hh'))\n",
        "                            minute = int(station.get('mm'))\n",
        "                            observation_time_lst = datetime(year, month, day, hour, minute).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        except (ValueError, TypeError):\n",
        "                            observation_time_lst = \"N/A\"\n",
        "\n",
        "                        extracted_data[param_name] = {\n",
        "                            'value': param_value,\n",
        "                            'unit': measurement_unit.strip(),\n",
        "                            'station_id': station_id,\n",
        "                            'station_latitude': station.get('LAT'),\n",
        "                            'station_longitude': station.get('LON'),\n",
        "                            'time_LST': observation_time_lst,\n",
        "                            'distance_miles': station_distance\n",
        "                            }\n",
        "                        parameters_found[param_name] = True\n",
        "                    else:\n",
        "                       all_found_for_for_this_station = False\n",
        "            if all(parameters_found.values()):\n",
        "                print(\"All requested parameters found within the 80-mile radius.\")\n",
        "                break # exits loop when all parameters are found\n",
        "        else:\n",
        "            break # stops when exceeds radius\n",
        "\n",
        "    # count the number of parameters found\n",
        "    num_parameters_found = sum(parameters_found.values())\n",
        "    total_parameters = len(requested_parameters)\n",
        "    print(f\"Found data for {num_parameters_found} out of {total_parameters} parameters within the {search_radius}-mile radius.\")\n",
        "\n",
        "    # identify missing parameters\n",
        "    missing_params = [param for param, found in parameters_found.items() if not found]\n",
        "\n",
        "    # structure json file\n",
        "    final_output = {\n",
        "        \"user_location\": {\n",
        "            \"latitude\": latitude,\n",
        "            \"longitude\": longitude,\n",
        "            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "        },\n",
        "        \"extracted_buoy_data\": extracted_data,\n",
        "        \"search_radius_miles\": search_radius,\n",
        "        \"missing_parameters\": missing_params,\n",
        "        \"searched_stations_within_radius\": searched_stations\n",
        "    }\n",
        "\n",
        "    # save json file\n",
        "    if final_output:\n",
        "        output_filename = \"user_data_noaa_buoy.json\"\n",
        "        try:\n",
        "            with open(output_filename, 'w') as f:\n",
        "                json.dump(final_output, f, indent=4)\n",
        "            print(f\"\\nNearest buoy data saved to {output_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving nearest buoy data to JSON: {e}\")\n",
        "    else:\n",
        "        print(\"No nearest buoy data available to save.\")\n",
        "else:\n",
        "    print(\"Buoy data is not available to extract data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gaYomvD7Yb7"
      },
      "source": [
        "### CO-OPS Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "6b982d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89766d82-c92c-4fcb-a822-e4f80cef7b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 301 stations from: https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations.json\n",
            "Fetched 381 stations from: https://opendap.co-ops.nos.noaa.gov/stations/stationsXML.jsp\n",
            "\n",
            "CO-OPS stations saved to dataset_noaa_coops.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "stations_json_url = \"https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations.json\"\n",
        "stations_xml_url = \"https://opendap.co-ops.nos.noaa.gov/stations/stationsXML.jsp\"\n",
        "\n",
        "try:\n",
        "    # stations from JSON source\n",
        "    response = requests.get(stations_json_url)\n",
        "    response.raise_for_status()\n",
        "    stations_data = response.json()\n",
        "\n",
        "    if stations_data and 'stations' in stations_data and isinstance(stations_data['stations'], list):\n",
        "        stations_df = pd.DataFrame(stations_data['stations'])\n",
        "        print(f\"Fetched {len(stations_df)} stations from: {stations_json_url}\")\n",
        "\n",
        "        selected_columns_df = stations_df[['id', 'name', 'state', 'lng', 'lat']].copy()\n",
        "        selected_columns_df = selected_columns_df.rename(columns={\n",
        "            'id': 'station_id',\n",
        "            'name': 'Station Name',\n",
        "            'state': 'State',\n",
        "            'lng': 'Longitude',\n",
        "            'lat': 'Latitude'\n",
        "        })\n",
        "\n",
        "        selected_columns_df['City, State'] = selected_columns_df['Station Name'] + ', ' + selected_columns_df['State']\n",
        "        formatted_stations_df = selected_columns_df[['station_id', 'City, State', 'Longitude', 'Latitude']]\n",
        "    else:\n",
        "        print(\"JSON data does not contain a list under the key 'stations'.\")\n",
        "        stations_df = pd.DataFrame()\n",
        "        formatted_stations_df = pd.DataFrame()\n",
        "\n",
        "    # stations from XML source\n",
        "    xml_response = requests.get(stations_xml_url)\n",
        "    xml_response.raise_for_status()\n",
        "    soup = BeautifulSoup(xml_response.content, \"xml\")\n",
        "\n",
        "    xml_stations = []\n",
        "    for st in soup.find_all(\"station\"):\n",
        "        xml_stations.append({\n",
        "            \"station_id\": st.get(\"ID\"),\n",
        "            \"Owner\": st.get(\"Owner\"),\n",
        "            \"Type\": st.get(\"Type\"),\n",
        "            \"Status\": st.get(\"Status\")\n",
        "        })\n",
        "\n",
        "    xml_df = pd.DataFrame(xml_stations)\n",
        "    print(f\"Fetched {len(xml_df)} stations from: {stations_xml_url}\")\n",
        "\n",
        "    # merge data\n",
        "    merged_nearby_stations_df = pd.merge(formatted_stations_df, xml_df, on=\"station_id\", how=\"left\")\n",
        "\n",
        "    # remove empty 'Owner', 'Type', and 'Status' columns\n",
        "    columns_to_drop = ['Owner', 'Type', 'Status']\n",
        "    merged_nearby_stations_df = merged_nearby_stations_df.drop(columns=columns_to_drop, errors='ignore') # if columns don't exist\n",
        "\n",
        "    # save json file\n",
        "    output_filename = \"dataset_noaa_coops.json\"\n",
        "    try:\n",
        "        merged_data_json = merged_nearby_stations_df.to_dict(orient='records')\n",
        "\n",
        "        # structure json file\n",
        "        output_data = {\n",
        "            \"datasets\": {\n",
        "                \"sources\": [stations_json_url, stations_xml_url],\n",
        "                \"total_stations_listed\": len(merged_nearby_stations_df) if not merged_nearby_stations_df.empty else 0,\n",
        "                \"description\": {\n",
        "                  \"about\": \"Maintained by the National Oceanic and Atmospheric Administration's (NOAA), Center for Operational Oceanographic Products and Services (CO-OPS). Provided is a list of active National Water Level Observation Network (NWLON) stations with some sensor configurations. More information can be found through https://opendap.co-ops.nos.noaa.gov/stations/index.jsp\"\n",
        "                },\n",
        "            \"station_list\": merged_data_json\n",
        "            },\n",
        "        }\n",
        "\n",
        "        with open(output_filename, 'w') as f:\n",
        "            json.dump(output_data, f, indent=4)\n",
        "        print(f\"\\nCO-OPS stations saved to {output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving merged CO-OPS station data to JSON: {e}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    stations_df = pd.DataFrame()\n",
        "    formatted_stations_df = pd.DataFrame()\n",
        "    merged_nearby_stations_df = pd.DataFrame()\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    stations_df = pd.DataFrame()\n",
        "    formatted_stations_df = pd.DataFrame()\n",
        "    merged_nearby_stations_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nearest Stations"
      ],
      "metadata": {
        "id": "PcHeXSXPlVfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.distance import geodesic\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# confirm user location\n",
        "if 'latitude' not in globals() or latitude is None or 'longitude' not in globals() or longitude is None:\n",
        "    print(\"User location (latitude or longitude) not available. Cannot filter stations by proximity.\")\n",
        "elif 'formatted_stations_df' not in globals() or formatted_stations_df.empty:\n",
        "    print(\"Formatted stations data is not available. Cannot filter stations.\")\n",
        "else:\n",
        "    user_coords = (latitude, longitude)\n",
        "    proximity_miles = 80\n",
        "\n",
        "    # calculate distance from user to each station\n",
        "    stations_for_distance_calc = formatted_stations_df.copy()\n",
        "    stations_for_distance_calc['Latitude'] = pd.to_numeric(stations_for_distance_calc['Latitude'], errors='coerce')\n",
        "    stations_for_distance_calc['Longitude'] = pd.to_numeric(stations_for_distance_calc['Longitude'], errors='coerce')\n",
        "\n",
        "    stations_for_distance_calc.dropna(subset=['Latitude', 'Longitude'], inplace=True) # drop lat/lon rows that could not be converted to numeric\n",
        "\n",
        "    def calculate_distance_miles(row):\n",
        "        station_coords = (row['Latitude'], row['Longitude'])\n",
        "        try:\n",
        "            return geodesic(user_coords, station_coords).miles\n",
        "        except ValueError:\n",
        "            return float('inf') # handles invalid coords\n",
        "\n",
        "    stations_for_distance_calc['distance_miles'] = stations_for_distance_calc.apply(calculate_distance_miles, axis=1)\n",
        "\n",
        "    # filter for stations within the specified proximity\n",
        "    merged_nearby_stations_df = stations_for_distance_calc[stations_for_distance_calc['distance_miles'] <= proximity_miles].sort_values(by='distance_miles')\n",
        "\n",
        "    if not merged_nearby_stations_df.empty:\n",
        "        merged_nearby_stations_df['distance_miles'] = merged_nearby_stations_df['distance_miles'].round(2)\n",
        "\n",
        "        # save json file\n",
        "        output_filename = \"user_data_noaa_coops_stations.json\"\n",
        "        try:\n",
        "            nearby_stations_json = merged_nearby_stations_df.to_dict(orient='records')\n",
        "\n",
        "            # structure json file\n",
        "            output_data = {\n",
        "                \"user_location\": {\n",
        "                    \"latitude\": latitude,\n",
        "                    \"longitude\": longitude,\n",
        "                    \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "                },\n",
        "                \"coops_stations\": {\n",
        "                    \"source_url\": stations_json_url,\n",
        "                    \"within_proximity_miles\": proximity_miles,\n",
        "                    \"total_stations_within_proximity\": len(merged_nearby_stations_df),\n",
        "                    \"nearby\": {\n",
        "                        \"stations\": merged_nearby_stations_df.to_dict(orient='records')\n",
        "                    },\n",
        "                },\n",
        "            }\n",
        "\n",
        "            with open(output_filename, 'w') as f:\n",
        "                json.dump(output_data, f, indent=4)\n",
        "            print(f\"Nearby CO-OPS stations saved to {output_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving nearby CO-OPS station data to JSON: {e}\")\n",
        "    else:\n",
        "        print(\"No CO-OPS stations found within the specified proximity.\")"
      ],
      "metadata": {
        "id": "GwfMQuXxkKtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b93d8ef-bc4b-47be-b5e2-822bfff32ad1"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearby CO-OPS stations saved to user_data_noaa_coops_stations.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tide Predictions"
      ],
      "metadata": {
        "id": "PqQHIozNutOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch tide predictions from CO-OPS API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "if 'merged_nearby_stations_df' not in globals() or merged_nearby_stations_df.empty:\n",
        "    print(\"Nearby CO-OPS stations are unavailable at this time.\")\n",
        "else:\n",
        "    today = datetime.now() # date range for yesterday and today\n",
        "    yesterday = today - timedelta(days=1)\n",
        "\n",
        "    begin_date_str = yesterday.strftime('%Y%m%d')\n",
        "    end_date_str = today.strftime('%Y%m%d') # format string YYYYMMDD\n",
        "\n",
        "    tide_predictions_url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "\n",
        "    about_tides = \"Tides refer to the rising and falling of the sea. Which is caused by the gravitational pull of the moon and the sun. Tides are very long-period waves that move through the ocean and progress toward the coastlines where they appear as the regular rise and fall of the sea surface. Tide predictions provide the times and heights for the astronomical tides. Predictions are based on the analysis of data collected at coastal locations maintained by the National Oceanic and Atmospheric Administration's (NOAA), Center for Operational Oceanographic Products and Services (CO-OPS). CO-OPS maintains the National Water Level Observation Network (NWLON), an observation network with more than 200 permanent water level stations on the coasts and Great Lakes.This system allows NOAA to provide the official tidal predictions for the nation. Accurate water level data is critical for safe and efficient marine navigation and for the protection of infrastructure along the coast.Harmonic stations are locations with enough long-term tide data to establish harmonic constants and tidal datums. Predictions for these stations are based solely on the analysis of that data. Because predictions at these locations are based on harmonic constants, predictions can be generated for any interval, and can be adjusted to different tidal datums. CO-OPS has preselected the most common intervals (hourly, 15- and 6-minute) for data queries. Learn more through https://tidesandcurrents.noaa.gov/water_level_info.html or https://tidesandcurrents.noaa.gov/education/tech-assist/training/user-guides/assets/pdfs/Tide_Predictions_User_Guide_v4.pdf\"\n",
        "\n",
        "    # initialize variables\n",
        "    tide_predictions_result = None\n",
        "    found_data = False\n",
        "    nearest_station_info = None\n",
        "\n",
        "    # iterate through nearby stations to find one with tide predictions\n",
        "    for index, station in merged_nearby_stations_df.iterrows():\n",
        "        nearest_station_id = station['station_id']\n",
        "\n",
        "        # API request parameters\n",
        "        params = {\n",
        "            'product': 'predictions',\n",
        "            'application': 'NOS.COOPS.TAC.WL',\n",
        "            'begin_date': begin_date_str,\n",
        "            'end_date': end_date_str,\n",
        "            'datum': 'MLLW',\n",
        "            'station': nearest_station_id,\n",
        "            'time_zone': 'lst_ldt', # local time with daylight saving\n",
        "            'units': 'english',\n",
        "            'interval': '', # leave interval empty for all predictions\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(tide_predictions_url, params=params)\n",
        "            if response.status_code == 200:\n",
        "\n",
        "                 # parse the JSON response\n",
        "                 tide_predictions_data = response.json()\n",
        "                 tide_predictions_result = tide_predictions_data\n",
        "\n",
        "                 # check if data is available in the response\n",
        "                 if tide_predictions_result and 'predictions' in tide_predictions_result and tide_predictions_result['predictions']:\n",
        "\n",
        "                    nearest_station_info = {\n",
        "                        \"station_id\": nearest_station_id,\n",
        "                        \"station_name\": station.get('City, State', 'N/A'),\n",
        "                        \"distance_miles\": round(station.get('distance_miles', float('inf')), 2),\n",
        "                        \"latitude\": station.get('Latitude', 'N/A'),\n",
        "                        \"longitude\": station.get('Longitude', 'N/A'),\n",
        "                        \"source_url\": response.url\n",
        "                    }\n",
        "                    found_data = True\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Error fetching tide predictions from station {nearest_station_id}: Status code {response.status_code}\")\n",
        "                print(f\"Response content:\\n{response.text}\")\n",
        "                tide_predictions_result = None # store None if fetch fails\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error during tide prediction request for station {nearest_station_id}: {e}\")\n",
        "            tide_predictions_result = None # store None if request fails\n",
        "\n",
        "    # check if data was found after iterating through stations\n",
        "    if found_data:\n",
        "        reformatted_predictions = []\n",
        "        if tide_predictions_result and 'predictions' in tide_predictions_result:\n",
        "            predictions_list = tide_predictions_result['predictions']\n",
        "\n",
        "            if predictions_list:\n",
        "\n",
        "                # save json file\n",
        "                output_filename = \"user_data_noaa_coops_tide.json\"\n",
        "\n",
        "                try:\n",
        "                    reformatted_predictions = []\n",
        "                    for prediction in predictions_list:\n",
        "                        reformatted_predictions.append({\n",
        "                            \"time_LST\": prediction.get('t'),\n",
        "                            \"water_level_ft\": prediction.get('v')\n",
        "                        })\n",
        "\n",
        "                    # structure json file\n",
        "                    output_data = {\n",
        "                        \"user_location\": {\n",
        "                            \"latitude\": latitude,\n",
        "                            \"longitude\": longitude,\n",
        "                            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "                        },\n",
        "                        \"tide_predictions\": {\n",
        "                            \"background\": {\n",
        "                                \"about\": about_tides,\n",
        "                                \"nearest_station\": {\n",
        "                                  \"station_info\": nearest_station_info,\n",
        "                                  \"data\": {\n",
        "                                    \"predictions\": reformatted_predictions\n",
        "                                },\n",
        "                              },\n",
        "                            },\n",
        "                          },\n",
        "                        }\n",
        "\n",
        "                    with open(output_filename, 'w') as f:\n",
        "                        json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
        "                    print(f\"Tide prediction data saved to {output_filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving tide prediction data to JSON: {e}\")\n",
        "    else:\n",
        "        print(\"No tide predictions found for nearby CO-OPS stations.\")"
      ],
      "metadata": {
        "id": "3S7XReV2sYLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2952b4d6-6882-4e5a-821b-c0d71317f002"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tide prediction data saved to user_data_noaa_coops_tide.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Water Level"
      ],
      "metadata": {
        "id": "6vy-uZ1oxag5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch water level observations from CO-OPS API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "if 'merged_nearby_stations_df' not in globals() or merged_nearby_stations_df.empty:\n",
        "    print(\"Nearby CO-OPS stations data is not available. Cannot fetch water level data.\")\n",
        "else:\n",
        "    end_date = datetime.now()\n",
        "    begin_date = end_date - timedelta(days=1) # last 24 hours date range\n",
        "\n",
        "    begin_date_str = begin_date.strftime('%Y%m%d') # string format YYYYMMDD\n",
        "    end_date_str = end_date.strftime('%Y%m%d')\n",
        "\n",
        "    water_level_url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "\n",
        "    # initialize variables\n",
        "    water_level_result = None\n",
        "    found_data = False\n",
        "\n",
        "    # iterate through nearby stations to find one with water level data\n",
        "    for index, station in merged_nearby_stations_df.iterrows():\n",
        "        nearest_station_id = station['station_id']\n",
        "\n",
        "        # API request parameters\n",
        "        params = {\n",
        "            'product': 'water_level',\n",
        "            'application': 'NOS.COOPS.TAC.WL',\n",
        "            'begin_date': begin_date_str,\n",
        "            'end_date': end_date_str,\n",
        "            'datum': 'MLLW',\n",
        "            'station': nearest_station_id,\n",
        "            'time_zone': 'lst', # using local standard time\n",
        "            'units': 'english',\n",
        "            'format': 'json',\n",
        "            'interval': '6', # updates data at 6-minute intervals\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(water_level_url, params=params)\n",
        "            if response.status_code == 200:\n",
        "\n",
        "                # parse the json response\n",
        "                water_level_data = response.json()\n",
        "                water_level_result = water_level_data\n",
        "\n",
        "                # check if data is available in the response\n",
        "                if water_level_result and 'data' in water_level_result and water_level_result['data']:\n",
        "\n",
        "                    # reformat keys\n",
        "                    reformatted_data = []\n",
        "                    for entry in water_level_result['data']:\n",
        "                        reformatted_entry = {\n",
        "                            \"time_LST\": entry.get('t'),\n",
        "                            \"water_level_ft\": entry.get('v'),\n",
        "                            \"sigma\": entry.get('s'),\n",
        "                            \"quality_flags\": entry.get('f'),\n",
        "                            \"quality_control_flag\": entry.get('q')\n",
        "                        }\n",
        "                        reformatted_data.append(reformatted_entry)\n",
        "\n",
        "                    # structure json file\n",
        "                    output_data = {\n",
        "                        \"user_location\": {\n",
        "                            \"latitude\": latitude,\n",
        "                            \"longitude\": longitude,\n",
        "                            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "                        },\n",
        "                        \"water_level\": {\n",
        "                            \"station_info\": {\n",
        "                                \"station_id\": nearest_station_id,\n",
        "                                \"station_name\": station.get('City, State', 'N/A'),\n",
        "                                \"distance_miles\": round(station.get('distance_miles', float('inf')), 2),\n",
        "                                \"latitude\": station.get('Latitude', 'N/A'),\n",
        "                                \"longitude\": station.get('Longitude', 'N/A'),\n",
        "                                \"source_url\": response.url\n",
        "                            },\n",
        "                              \"data\": reformatted_data\n",
        "                        },\n",
        "                    }\n",
        "\n",
        "                    # save json file\n",
        "                    output_filename = \"user_data_noaa_coops_waterlevel.json\"\n",
        "                    try:\n",
        "                        with open(output_filename, 'w') as f:\n",
        "                            json.dump(output_data, f, indent=4)\n",
        "                        print(f\"Water level observation data saved to {output_filename}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error saving water level observation data to JSON: {e}\")\n",
        "\n",
        "                    found_data = True\n",
        "                    break\n",
        "                else:\n",
        "                     print(f\"No water level data available for station {nearest_station_id}.\")\n",
        "            else:\n",
        "                print(f\"Error fetching water level data: Status code {response.status_code}\")\n",
        "                print(f\"Response content:\\n{response.text}\")\n",
        "                water_level_result = None # store None if fetch fails\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error during water level data request for station {nearest_station_id}: {e}\")\n",
        "            water_level_result = None # store None if request fails\n",
        "    if not found_data:\n",
        "        print(\"Could not find water level observations from any nearby CO-OPS station.\")"
      ],
      "metadata": {
        "id": "RYReABw0vkt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f16ee66-acc6-4070-d117-b4ec84fdb6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Water level observation data saved to user_data_noaa_coops_waterlevel.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Air Temp"
      ],
      "metadata": {
        "id": "_WBSjf3EzW_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch air temperature data from CO-OPS API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "if 'merged_nearby_stations_df' not in globals() or merged_nearby_stations_df.empty:\n",
        "    print(\"Nearby CO-OPS stations data is not available. Cannot fetch air temperature data.\")\n",
        "else:\n",
        "    today = datetime.now()\n",
        "    begin_date_str = today.strftime('%Y%m%d')\n",
        "    end_date_str = today.strftime('%Y%m%d')\n",
        "\n",
        "    air_temp_url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "\n",
        "    # initialize variables\n",
        "    air_temp_result = None\n",
        "    found_data = False\n",
        "    nearest_station_info = None\n",
        "\n",
        "    # iterate through nearby stations to find one with air temp data\n",
        "    for index, station in merged_nearby_stations_df.iterrows():\n",
        "        nearest_station_id = station['station_id']\n",
        "\n",
        "        # API request parameters\n",
        "        params = {\n",
        "            'product': 'air_temperature',\n",
        "            'application': 'NOS.COOPS.TAC.MET',\n",
        "            'begin_date': begin_date_str,\n",
        "            'end_date': end_date_str,\n",
        "            'station': nearest_station_id,\n",
        "            'units': 'metric',\n",
        "            'time_zone': 'lst',\n",
        "            'format': 'json',\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(air_temp_url, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                air_temp_data = response.json()\n",
        "                if air_temp_data and 'data' in air_temp_data and air_temp_data['data']:\n",
        "                    air_temp_result = air_temp_data\n",
        "\n",
        "                    nearest_station_info = {\n",
        "                        \"station_id\": nearest_station_id,\n",
        "                        \"station_name\": station.get('City, State', 'N/A'),\n",
        "                        \"distance_miles\": round(station.get('distance_miles', float('inf')), 2),\n",
        "                        \"latitude\": station.get('Latitude', 'N/A'),\n",
        "                        \"longitude\": station.get('Longitude', 'N/A'),\n",
        "                        \"source_url\": response.url\n",
        "                    }\n",
        "                    found_data = True\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Error fetching air temperature data from station {nearest_station_id}: Status code {response.status_code}\")\n",
        "                print(f\"Response content:\\n{response.text}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error during air temperature data request for station {nearest_station_id}: {e}\")\n",
        "\n",
        "    if found_data:\n",
        "        if air_temp_result and 'data' in air_temp_result:\n",
        "            air_temp_list = air_temp_result['data']\n",
        "            if air_temp_list:\n",
        "\n",
        "                # save json file\n",
        "                output_filename = \"user_data_noaa_coops_airtemp.json\"\n",
        "                try:\n",
        "                    reformatted_data = []\n",
        "                    for entry in air_temp_list:\n",
        "                        temp_celsius = float(entry.get('v')) if entry.get('v') is not None else None\n",
        "                        temp_fahrenheit = (temp_celsius * 9/5) + 32 if temp_celsius is not None else None\n",
        "\n",
        "                        reformatted_data.append({\n",
        "                            \"time_LST\": entry.get('t'),\n",
        "                            \"temp_fahrenheit\": temp_fahrenheit,\n",
        "                            \"quality_flags\": entry.get('f')\n",
        "                        })\n",
        "\n",
        "                    # structure json file\n",
        "                    output_data = {\n",
        "                        \"user_location\": {\n",
        "                            \"latitude\": latitude,\n",
        "                            \"longitude\": longitude,\n",
        "                            \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "                        },\n",
        "                        \"air_temperature\": {\n",
        "                            \"station_info\": nearest_station_info,\n",
        "                            \"data\": reformatted_data\n",
        "                        },\n",
        "                    }\n",
        "\n",
        "                    with open(output_filename, 'w') as f:\n",
        "                        json.dump(output_data, f, indent=4)\n",
        "                    print(f\"Air temperature data saved to {output_filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving air temperature data to JSON: {e}\")\n",
        "    else:\n",
        "        print(\"No air temperatures found for nearby CO-OPS stations.\")"
      ],
      "metadata": {
        "id": "RTHz1uzMzWsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105048fb-508b-4aed-c771-ba4529e6d8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Air temperature data saved to user_data_noaa_coops_airtemp.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Water Temp"
      ],
      "metadata": {
        "id": "m0KH5ZoR3CDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch water temperature data from CO-OPS API\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "if 'merged_nearby_stations_df' not in globals() or merged_nearby_stations_df.empty:\n",
        "    print(\"Nearby station for CO-OPS data is not available. Cannot fetch water temperature data.\")\n",
        "else:\n",
        "    today = datetime.now()\n",
        "    begin_date_str = today.strftime('%Y%m%d')\n",
        "    end_date_str = today.strftime('%Y%m%d')\n",
        "\n",
        "    water_temp_url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "\n",
        "    # initialize variables\n",
        "    water_temp_result = None\n",
        "    found_data = False\n",
        "    nearest_station_info = None\n",
        "\n",
        "    # iterate through nearby stations to find one with data\n",
        "    for index, station in merged_nearby_stations_df.iterrows():\n",
        "        nearest_station_id = station['station_id']\n",
        "\n",
        "        # parameters for the API request\n",
        "        params = {\n",
        "            'product': 'water_temperature',\n",
        "            'application': 'NOS.COOPS.TAC.PHYSOCEAN',\n",
        "            'begin_date': begin_date_str,\n",
        "            'end_date': end_date_str,\n",
        "            'station': nearest_station_id,\n",
        "            'units': 'english',\n",
        "            'time_zone': 'lst',\n",
        "            'format': 'json',\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(water_temp_url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                water_temp_data = response.json()\n",
        "                if water_temp_data and 'data' in water_temp_data and water_temp_data['data']:\n",
        "                    water_temp_result = water_temp_data\n",
        "\n",
        "                    nearest_station_info = {\n",
        "                        \"station_id\": nearest_station_id,\n",
        "                        \"station_name\": station.get('City, State', 'N/A'),\n",
        "                        \"distance_miles\": round(station.get('distance_miles', float('inf')), 2),\n",
        "                        \"latitude\": station.get('Latitude', 'N/A'),\n",
        "                        \"longitude\": station.get('Longitude', 'N/A'),\n",
        "                        \"source_url\": response.url\n",
        "                    }\n",
        "                    found_data = True\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"Error fetching water temperature data from station {nearest_station_id}: Status code {response.status_code}\")\n",
        "                print(f\"Response content:\\n{response.text}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error during water temperature data request for station {nearest_station_id}: {e}\")\n",
        "\n",
        "    if found_data:\n",
        "        reformatted_water_temp = []\n",
        "        if water_temp_result and 'data' in water_temp_result:\n",
        "            for entry in water_temp_result['data']:\n",
        "                reformatted_water_temp.append({\n",
        "                    \"time_LST\": entry.get('t'),\n",
        "                    \"value\": entry.get('v'), # do not change name\n",
        "                    \"quality_flags\": entry.get('f'),\n",
        "                    \"quality_control\": entry.get('q')\n",
        "                })\n",
        "\n",
        "        # structure json file\n",
        "        output_data = {\n",
        "            \"user_location\": {\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude,\n",
        "                \"date\": datetime.now().strftime('%Y-%m-%d')\n",
        "            },\n",
        "            \"water_temperature\": {\n",
        "                \"station_info\": nearest_station_info,\n",
        "                \"data\": reformatted_water_temp\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # save json file\n",
        "        output_filename = \"user_data_noaa_coops_watertemp.json\"\n",
        "        try:\n",
        "            with open(output_filename, 'w') as f:\n",
        "                json.dump(output_data, f, indent=4)\n",
        "            print(f\"Water temperature data saved to {output_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving water temperatures to JSON: {e}\")\n",
        "    else:\n",
        "        print(\"No water temperatures found for nearby CO-OPS stations.\")"
      ],
      "metadata": {
        "id": "LYf0EYI-BEmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63850a59-90e6-4366-92bd-518bfdd3e921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Water temperature data saved to user_data_noaa_coops_watertemp.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources"
      ],
      "metadata": {
        "id": "a4KcMNo1vLE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9erFsmxVG_5"
      },
      "outputs": [],
      "source": [
        "# currents predictions:\n",
        "# https://tidesandcurrents.noaa.gov/currents_info.html\n",
        "# https://tidesandcurrents.noaa.gov/education/tech-assist/training/user-guides/assets/pdfs/Current_Predictions_User_Guide_v5.pdf\n",
        "# https://tidesandcurrents.noaa.gov/noaacurrents/assets/docs/Tidal_Current_Prediction_Uncertainty.pdf\n",
        "\n",
        "# metadata sources:\n",
        "# https://tidesandcurrents.noaa.gov/cdata/StationList?type=Current+Data&filter=active\n",
        "# https://tidesandcurrents.noaa.gov/cdata/StationList?type=Current+Data&filter=historic\n",
        "# https://tidesandcurrents.noaa.gov/waterlevels.html?id=9447130\n",
        "# These raw data have not been subjected to the National Ocean Service's quality control or quality assurance procedures and do not meet the criteria and standards of official National Ocean Service data. They are released for limited public use as preliminary data to be used only with appropriate caution.\n",
        "# now use CO-OPS sensor API to retrieve observations and predictions from CO-OPS stations listed here\n",
        "# source: https://api.tidesandcurrents.noaa.gov/api/prod/#requestResponse\n",
        "\n",
        "# SOS sea surface temp - https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/ / https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&request=GetObservation&version=1.0.0&observedProperty=sea_water_temperature&offering=urn:ioos:station:NOAA.NOS.CO-OPS:{station_id}&responseFormat=text/xml;subtype=\"om/1.0.0/profiles/ioos_sos/1.0\"&unit=Fahrenheit\n",
        "\n",
        "# SOS salinity - https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&request=GetObservation&version=1.0.0&observedProperty=sea_water_salinity&offering=urn:ioos:station:NOAA.NOS.CO-OPS:{station_id}&responseFormat=text/xml;subtype=\"om/1.0.0/profiles/ioos_sos/1.0\"\n",
        "\n",
        "# SOS water surface height (data_types: PreliminaryOneMinute, PreliminarySixMinute) - https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&request=GetObservation&version=1.0.0&observedProperty=water_surface_height_above_reference_datum&offering=urn:ioos:station:NOAA.NOS.CO-OPS:{station_id}&responseFormat=text/xml;subtype=\"om/1.0.0/profiles/ioos_sos/1.0\"&dataType={data_type}&unit=Meters\n",
        "\n",
        "# SOS verified (6-min) water surface height - https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&request=GetObservation&version=1.0.0&observedProperty=water_surface_height_above_reference_datum&offering=urn:ioos:station:NOAA.NOS.CO-OPS:{station_id}&responseFormat=text/xml;subtype=\"om/1.0.0/profiles/ioos_sos/1.0\"&eventTime={2025-06-01T00:00:00Z}/{2025-06-01T23:59:00Z}&dataType=VerifiedSixMinute&unit=Meters\n",
        "\n",
        "# SOS verified high/low water surface height - https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&request=GetObservation&version=1.0.0&observedProperty=water_surface_height_above_reference_datum&offering=urn:ioos:station:NOAA.NOS.CO-OPS:8454000&responseFormat=text/xml;subtype=\"om/1.0.0/profiles/ioos_sos/1.0\"&eventTime=2025-06-01T00:00:00Z/2025-06-05T23:59:00Z&dataType=VerifiedHighLow&unit=Meters\n",
        "\n",
        "# CO-OPS SOAP Web Services - https://opendap.co-ops.nos.noaa.gov/axis/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b91637f"
      },
      "source": [
        "### Weather Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4OYyjH-DeNd"
      },
      "source": [
        "#### Hourly Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa8a4827"
      },
      "outputs": [],
      "source": [
        "# hourly temperature forecast line graph -------------------------------------\n",
        "\n",
        "%pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import date\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        temperatures = [period['temperature'] for period in hourly_periods_today]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, temperatures, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(f\"Temperature ({hourly_periods_today[0]['temperatureUnit']})\")\n",
        "        plt.title(f\"Hourly Temperature Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # format x-axis to show time with appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # format tick locations\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent label overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly apparent temperature forecast line graph ----------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    today = date.today() # filter for the current date\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "\n",
        "        # use apparent temperature if available, otherwise use temperature\n",
        "        temperatures = []\n",
        "        for period in hourly_periods_today:\n",
        "            apparent_temp = period.get('apparentTemperature', {}).get('value')\n",
        "            if apparent_temp is not None:\n",
        "                temperatures.append(apparent_temp)\n",
        "            else:\n",
        "                temperatures.append(period.get('temperature'))\n",
        "\n",
        "        # determine the unit\n",
        "        temp_unit = hourly_periods_today[0].get('temperatureUnit', '')\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, temperatures, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(f\"Temperature ({temp_unit})\") # label reflects whats plotted\n",
        "        plt.title(f\"Hourly Temperature (Apparent if available) Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # format x-axis to show time with appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # format tick locations\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent label overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly cloud cover forecast line graph -------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract cloud cover - need to handle potential none values\n",
        "        cloud_cover_values = [period.get('cloudCover', {}).get('value') for period in hourly_periods_today]\n",
        "\n",
        "        # remove none values for plotting\n",
        "        times_filtered = [times[i] for i, val in enumerate(cloud_cover_values) if val is not None]\n",
        "        cloud_cover_values_filtered = [val for val in cloud_cover_values if val is not None]\n",
        "\n",
        "\n",
        "        if not cloud_cover_values_filtered:\n",
        "             print(\"Cloud Cover data not available for visualization for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times_filtered, cloud_cover_values_filtered, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(\"Cloud Cover (%)\")\n",
        "            plt.title(f\"Hourly Cloud Cover Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3) # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly precipitation forecast probability line graph -----------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract probability of precipitation\n",
        "        precipitation_probs = [period.get('probabilityOfPrecipitation', {}).get('value', 0) for period in hourly_periods_today]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, precipitation_probs, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Probability of Precipitation (%)\")\n",
        "        plt.title(f\"Hourly Probability of Precipitation Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # format x-axis to show only time and set appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly humidity forecast line graph ----------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # Extract relative humidity\n",
        "        humidity_values = [period.get('relativeHumidity', {}).get('value', 0) for period in hourly_periods_today]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, humidity_values, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Relative Humidity (%)\")\n",
        "        plt.title(f\"Hourly Relative Humidity Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # format the x-axis to show only time and set appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly visibility forecast line graph --------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract visibility\n",
        "        visibility_values = []\n",
        "        visibility_unit = '' # initialize unit\n",
        "        for period in hourly_periods_today:\n",
        "            visibility_data = period.get('visibility', {})\n",
        "            value = visibility_data.get('value')\n",
        "            if value is not None:\n",
        "                visibility_values.append(value)\n",
        "                # capture the unit from the first available data point\n",
        "                if not visibility_unit and visibility_data.get('unitCode'):\n",
        "                    visibility_unit = visibility_data.get('unitCode')\n",
        "            else:\n",
        "                visibility_values.append(None) # append if data is missing\n",
        "\n",
        "        # remove None values for plotting\n",
        "        times_filtered = [times[i] for i, val in enumerate(visibility_values) if val is not None]\n",
        "        visibility_values_filtered = [val for val in visibility_values if val is not None]\n",
        "\n",
        "\n",
        "        if not visibility_values_filtered:\n",
        "             print(\"Visibility data not available for visualization for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times_filtered, visibility_values_filtered, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            # set ylabel based on the captured unit\n",
        "            ylabel = \"Visibility\"\n",
        "            if visibility_unit:\n",
        "                 ylabel += f\" ({visibility_unit})\"\n",
        "\n",
        "            plt.ylabel(ylabel)\n",
        "            plt.title(f\"Hourly Visibility Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # auto-format for labels overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly dewpoint forecast line graph ----------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract dewpoint - need to handle potential None values\n",
        "        dewpoint_values = [period.get('dewpoint', {}).get('value') for period in hourly_periods_today]\n",
        "\n",
        "        # convert dewpoint from C to F if temperature unit is F\n",
        "        if hourly_periods_today and hourly_periods_today[0].get('temperatureUnit') == 'F':\n",
        "             dewpoint_values = [(d * 9/5) + 32 if d is not None else None for d in dewpoint_values]\n",
        "             dewpoint_unit = 'F'\n",
        "        elif hourly_periods_today and hourly_periods_today[0].get('dewpoint') and hourly_periods_today[0]['dewpoint'].get('unitCode'):\n",
        "             dewpoint_unit = hourly_periods_today[0]['dewpoint'].get('unitCode')\n",
        "        else:\n",
        "             dewpoint_unit = ''\n",
        "\n",
        "\n",
        "        # remove None values for plotting\n",
        "        times_filtered = [times[i] for i, val in enumerate(dewpoint_values) if val is not None]\n",
        "        dewpoint_values_filtered = [val for val in dewpoint_values if val is not None]\n",
        "\n",
        "\n",
        "        if not dewpoint_values_filtered:\n",
        "             print(\"Dewpoint data not available for visualization for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times_filtered, dewpoint_values_filtered, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(f\"Dewpoint ({dewpoint_unit})\")\n",
        "            plt.title(f\"Hourly Dewpoint Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly wind speed forecast line graph --------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract wind speed\n",
        "        wind_speeds = []\n",
        "        for period in hourly_periods_today:\n",
        "            # for cases when windSpeed is 0\n",
        "            speed_str = period.get('windSpeed', '0 mph').split(' ')[0]\n",
        "            try:\n",
        "                wind_speeds.append(int(speed_str))\n",
        "            except ValueError:\n",
        "                # for cases when windSpeed is a range; takes the first number\n",
        "                 try:\n",
        "                     wind_speeds.append(int(speed_str.split(' to ')[0]))\n",
        "                 except ValueError:\n",
        "                     wind_speeds.append(0) # default to 0 if parsing fails\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, wind_speeds, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Wind Speed (mph)\")\n",
        "        plt.title(f\"Hourly Wind Speed Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # format x-axis to show only time and set appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly wind gust forecast line graph ---------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract windGust\n",
        "        wind_gusts = []\n",
        "        for period in hourly_periods_today:\n",
        "            gust_str = period.get('windGust')\n",
        "            if gust_str: # check if windGust exists and is not None/empty\n",
        "                try:\n",
        "                    # for when windGust is a number or a range\n",
        "                    if ' to ' in gust_str:\n",
        "                        wind_gusts.append(int(gust_str.split(' to ')[0]))\n",
        "                    else:\n",
        "                        # assume unit is separated by space\n",
        "                        wind_gusts.append(int(gust_str.split(' ')[0]))\n",
        "                except (ValueError, IndexError):\n",
        "                    wind_gusts.append(0) # default to 0 if parsing fails\n",
        "            else:\n",
        "                wind_gusts.append(0) # append 0 if windGust is missing\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, wind_gusts, marker='o')\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Wind Gust (mph)\") # changed label to Wind Gust\n",
        "        plt.title(f\"Hourly Wind Gust Forecast for {today.strftime('%Y-%m-%d')}\") # changed title\n",
        "\n",
        "        # set the y-axis limit to start at 0\n",
        "        plt.ylim(bottom=0)\n",
        "\n",
        "        # format x-axis to show only time and set appropriate intervals\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%H:%M')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        import numpy as np\n",
        "        hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "        ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly sky cover forecast line graph --------------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract skyCover - need to handle potential None values\n",
        "        sky_cover_values = [period.get('skyCover', {}).get('value') for period in hourly_periods_today]\n",
        "\n",
        "        # remove None values for plotting\n",
        "        times_filtered = [times[i] for i, val in enumerate(sky_cover_values) if val is not None]\n",
        "        sky_cover_values_filtered = [val for val in sky_cover_values if val is not None]\n",
        "\n",
        "        if not sky_cover_values_filtered:\n",
        "             print(\"Sky Cover data not available for visualization for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times_filtered, sky_cover_values_filtered, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(\"Sky Cover (%)\")\n",
        "            plt.title(f\"Hourly Sky Cover Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly heat / UV index forecast line graph ---------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract heat index - need to handle potential None values\n",
        "        heat_index_values = [period.get('heatIndex', {}).get('value') for period in hourly_periods_today]\n",
        "\n",
        "        # determine the unit if available\n",
        "        heat_index_unit = ''\n",
        "        if hourly_periods_today and hourly_periods_today[0].get('heatIndex') and hourly_periods_today[0]['heatIndex'].get('unitCode'):\n",
        "             heat_index_unit = hourly_periods_today[0]['heatIndex'].get('unitCode')\n",
        "\n",
        "\n",
        "        # remove None values for plotting\n",
        "        times_filtered = [times[i] for i, val in enumerate(heat_index_values) if val is not None]\n",
        "        heat_index_values_filtered = [val for val in heat_index_values if val is not None]\n",
        "\n",
        "\n",
        "        if not heat_index_values_filtered:\n",
        "             print(\"Heat Index data not available for visualization for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times_filtered, heat_index_values_filtered, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(f\"Heat Index ({heat_index_unit})\")\n",
        "            plt.title(f\"Hourly Heat Index Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly snowfall amount forecast line graph ---------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract snowfallAmount\n",
        "        snowfall_values = []\n",
        "        snowfall_unit = ''\n",
        "        for period in hourly_periods_today:\n",
        "            snowfall_data = period.get('snowfallAmount', {})\n",
        "            value = snowfall_data.get('value')\n",
        "            if value is not None:\n",
        "                snowfall_values.append(value)\n",
        "                # capture the unit from the first available data point\n",
        "                if not snowfall_unit and snowfall_data.get('unitCode'):\n",
        "                    snowfall_unit = snowfall_data.get('unitCode')\n",
        "            else:\n",
        "                snowfall_values.append(0) # append 0 if data is missing\n",
        "\n",
        "\n",
        "        if not any(snowfall_values): # check for any snowfall predictions\n",
        "             print(\"Snowfall data not available or no snowfall predicted for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times, snowfall_values, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(f\"Snowfall Amount ({snowfall_unit})\")\n",
        "            plt.title(f\"Hourly Snowfall Amount Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "\n",
        "# hourly ice accumulation forecast line graph --------------------------------\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # filter for the current date\n",
        "    today = date.today()\n",
        "    hourly_periods_today = [\n",
        "        period for period in hourly_periods\n",
        "        if datetime.fromisoformat(period['startTime']).date() == today\n",
        "    ]\n",
        "\n",
        "    if not hourly_periods_today:\n",
        "        print(\"No hourly forecast data available for the current date.\")\n",
        "    else:\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in hourly_periods_today]\n",
        "        # extract iceAccumulation\n",
        "        ice_accumulation_values = []\n",
        "        ice_accumulation_unit = ''\n",
        "        for period in hourly_periods_today:\n",
        "            ice_data = period.get('iceAccumulation', {})\n",
        "            value = ice_data.get('value')\n",
        "            if value is not None:\n",
        "                ice_accumulation_values.append(value)\n",
        "                # capture the unit from the first available data point\n",
        "                if not ice_accumulation_unit and ice_data.get('unitCode'):\n",
        "                    ice_accumulation_unit = ice_data.get('unitCode')\n",
        "            else:\n",
        "                # append 0 if data is missing\n",
        "                ice_accumulation_values.append(0)\n",
        "\n",
        "        # check if there is any ice accumulation predicted\n",
        "        if not any(ice_accumulation_values):\n",
        "             print(\"Ice accumulation data not available or no ice accumulation predicted for the current date.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(times, ice_accumulation_values, marker='o')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(f\"Ice Accumulation ({ice_accumulation_unit})\")\n",
        "            plt.title(f\"Hourly Ice Accumulation Forecast for {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # format x-axis to show only time and set appropriate intervals\n",
        "            ax = plt.gca()\n",
        "            formatter = mdates.DateFormatter('%H:%M')\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "            import numpy as np\n",
        "            hours = mdates.HourLocator(interval=3)  # set a tick every 3 hours\n",
        "            ax.xaxis.set_major_locator(hours)\n",
        "\n",
        "\n",
        "            plt.gcf().autofmt_xdate() # format to prevent label overlapping\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Hourly forecast data not available for visualization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kvmQNgtJRpK"
      },
      "source": [
        "#### Hourly Container +7-Day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de35e27c"
      },
      "outputs": [],
      "source": [
        "# hourly forecast container for 7-days\n",
        "\n",
        "import json\n",
        "from datetime import datetime # Import datetime for parsing the string timestamps\n",
        "\n",
        "\n",
        "if weather_data_result and 'hourly_forecast' in weather_data_result:\n",
        "    hourly_periods = weather_data_result['hourly_forecast']['properties']['periods']\n",
        "\n",
        "    # convert datetime objects to strings for JSON serialization\n",
        "    # for period in hourly_periods:\n",
        "    #     period['startTime'] = period['startTime'].isoformat()\n",
        "    #     period['endTime'] = period['endTime'].isoformat()\n",
        "\n",
        "    print(\"\\n--- Hourly Forecast ---\")\n",
        "    for period in hourly_periods:\n",
        "        # Parse the string timestamps into datetime objects for printing\n",
        "        start_time_dt = datetime.fromisoformat(period['startTime'])\n",
        "        print(f\"\\nTime: {start_time_dt.strftime('%Y-%m-%d %H:%M')}\")\n",
        "        print(f\"  Temperature: {period.get('temperature')}{period.get('temperatureUnit', '')}\")\n",
        "\n",
        "        if period.get('windChill') and period['windChill'].get('value') is not None:\n",
        "             print(f\"  Wind Chill: {period['windChill']['value']}{period['windChill'].get('unitCode', '')}\")\n",
        "\n",
        "        if period.get('heatIndex') and period['heatIndex'].get('value') is not None:\n",
        "             print(f\"  Heat Index: {period['heatIndex']['value']}{period['heatIndex'].get('unitCode', '')}\")\n",
        "        print(f\"  Wind Speed: {period.get('windSpeed', 'N/A')}\")\n",
        "        print(f\"  Wind Direction: {period.get('windDirection', 'N/A')}\")\n",
        "\n",
        "        if period.get('cloudCover') and period['cloudCover'].get('value') is not None:\n",
        "             print(f\"  Cloud Cover: {period['cloudCover']['value']}%\")\n",
        "\n",
        "        if period.get('probabilityOfPrecipitation') and period['probabilityOfPrecipitation'].get('value') is not None:\n",
        "             print(f\"  Probability of Precipitation: {period['probabilityOfPrecipitation']['value']}%\")\n",
        "\n",
        "        if period.get('dewpoint') and period['dewpoint'].get('value') is not None:\n",
        "             # convert dewpoint from C to F if temperature unit is F\n",
        "             dewpoint_value = period['dewpoint']['value']\n",
        "             dewpoint_unit = period['dewpoint'].get('unitCode', '')\n",
        "             if period.get('temperatureUnit') == 'F' and dewpoint_unit == 'wmoUnit:degC':\n",
        "                 dewpoint_value = (dewpoint_value * 9/5) + 32\n",
        "                 dewpoint_unit = 'F' # update unit to F\n",
        "\n",
        "             # if the original unit is already F (unlikely)\n",
        "             elif period.get('temperatureUnit') == 'F' and dewpoint_unit == 'wmoUnit:degF':\n",
        "                 dewpoint_unit = 'F'\n",
        "\n",
        "             print(f\"  Dewpoint: {dewpoint_value:.0f}{dewpoint_unit}\")\n",
        "\n",
        "\n",
        "        if period.get('relativeHumidity') and period['relativeHumidity'].get('value') is not None:\n",
        "             print(f\"  Relative Humidity: {period['relativeHumidity']['value']}%\")\n",
        "\n",
        "        print(f\"  Short Forecast: {period.get('shortForecast', 'N/A')}\")\n",
        "\n",
        "\n",
        "    # save hourly forecast data to JSON file\n",
        "    hourly_output_filename = \"noaa_hourly_7day_forecast.json\"\n",
        "    try:\n",
        "        with open(hourly_output_filename, 'w') as f:\n",
        "            json.dump(hourly_periods, f, indent=4)\n",
        "        print(f\"\\nHourly forecast data saved to {hourly_output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving hourly forecast data to JSON: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Hourly forecast data not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKrIlDv3qB2"
      },
      "source": [
        "#### 7-Day Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4be4555"
      },
      "outputs": [],
      "source": [
        "# 7-day temperature bar graph ------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "if weather_data_result and 'forecast' in weather_data_result:\n",
        "    forecast_periods = weather_data_result['forecast']['properties']['periods']\n",
        "\n",
        "    if not forecast_periods:\n",
        "        print(\"7-day forecast data not available for visualization.\")\n",
        "    else:\n",
        "        # extract times, temperatures, and daytime status for each period\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in forecast_periods]\n",
        "        temperatures = [period['temperature'] for period in forecast_periods]\n",
        "        is_daytime = [period['isDaytime'] for period in forecast_periods]\n",
        "\n",
        "        # eetermine the unit\n",
        "        temp_unit = forecast_periods[0].get('temperatureUnit', '')\n",
        "\n",
        "        # define colors for day and night\n",
        "        colors = ['skyblue' if day else 'darkblue' for day in is_daytime]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # plot bars with different colors based on daytime status\n",
        "        plt.bar(times, temperatures, width=0.4, color=colors)\n",
        "        plt.xlabel(\"Day\")\n",
        "        plt.ylabel(f\"Temperature ({temp_unit})\")\n",
        "        plt.title(\"7-Day Temperature Forecast\")\n",
        "\n",
        "        # format the x-axis to show days of the week\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%A')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # set tick locations (one tick per day)\n",
        "        ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"7-day forecast data not available for visualization.\")\n",
        "\n",
        "\n",
        "# 7-day precipitation probability line graph ---------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "if weather_data_result and 'forecast' in weather_data_result:\n",
        "    forecast_periods = weather_data_result['forecast']['properties']['periods']\n",
        "\n",
        "    if not forecast_periods:\n",
        "        print(\"7-day forecast data not available for visualization.\")\n",
        "    else:\n",
        "        # extract times and probability of precipitation for each period\n",
        "        times = [datetime.fromisoformat(period['startTime']) for period in forecast_periods]\n",
        "        # extract probability of precipitation, handling potential None values\n",
        "        precipitation_probs = [period.get('probabilityOfPrecipitation', {}).get('value', 0) for period in forecast_periods]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(times, precipitation_probs, marker='o')\n",
        "        plt.xlabel(\"Day\")\n",
        "        plt.ylabel(\"Probability of Precipitation (%)\")\n",
        "        plt.title(\"7-Day Probability of Precipitation Forecast\")\n",
        "\n",
        "        # format the x-axis to show days of the week\n",
        "        ax = plt.gca()\n",
        "        formatter = mdates.DateFormatter('%A')\n",
        "        ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # set tick locations (one tick per day)\n",
        "        ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "\n",
        "\n",
        "        plt.gcf().autofmt_xdate() # auto-format to prevent labels overlapping\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"7-day forecast data not available for visualization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbAeqA7eDSLi"
      },
      "source": [
        "#### 7-Day Container +Alerts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011a45ec"
      },
      "outputs": [],
      "source": [
        "# 7-day forecast container\n",
        "\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "if weather_data_result and 'forecast' in weather_data_result:\n",
        "    forecast_periods = weather_data_result['forecast']['properties']['periods']\n",
        "    print(\"--- 7-Day Forecast ---\")\n",
        "\n",
        "    # Print the total number of active weather alerts found\n",
        "    if parsed_alerts:\n",
        "        print(f\"Total active weather alerts found for {user_location}: {len(parsed_alerts)}\")\n",
        "    else:\n",
        "        print(f\"No active weather alerts found for {user_location}.\")\n",
        "\n",
        "    for period in forecast_periods:\n",
        "        # convert period times to datetime objects (assuming ISO 8601 format)\n",
        "        start_time = datetime.fromisoformat(period['startTime'])\n",
        "        end_time = datetime.fromisoformat(period['endTime'])\n",
        "\n",
        "        print(f\"\\n{period['name']}:\")\n",
        "        print(f\"  Temperature: {period.get('temperature')}{period.get('temperatureUnit', '')}\")\n",
        "        print(f\"  Short Forecast: {period.get('shortForecast', 'N/A')}\")\n",
        "        print(f\"  Detailed Forecast: {period.get('detailedForecast', 'N/A')}\")\n",
        "        print(f\"  Wind Speed: {period.get('windSpeed', 'N/A')}\")\n",
        "        print(f\"  Wind Direction: {period.get('windDirection', 'N/A')}\")\n",
        "\n",
        "        # include probability of precipitation\n",
        "        if period.get('probabilityOfPrecipitation') and period['probabilityOfPrecipitation'].get('value') is not None:\n",
        "             print(f\"  Probability of Precipitation: {period['probabilityOfPrecipitation']['value']}%\")\n",
        "\n",
        "        # check for relevant active alerts within forecast period\n",
        "        relevant_alerts = []\n",
        "        if parsed_alerts:\n",
        "            for alert in parsed_alerts:\n",
        "                alert_effective = datetime.fromisoformat(alert.get('effective')) if alert.get('effective') else None\n",
        "                alert_expires = datetime.fromisoformat(alert.get('expires')) if alert.get('expires') else None\n",
        "\n",
        "                # check if alert times overlap with the forecast period\n",
        "                if alert_effective and alert_expires:\n",
        "                    # an alert is relevant if it starts before or during the period and ends after or during the period\n",
        "                    if (alert_effective <= end_time and alert_expires >= start_time):\n",
        "                         relevant_alerts.append(alert)\n",
        "                # alert with no explicit end time\n",
        "                elif alert_effective and not alert_expires:\n",
        "                     if alert_effective <= end_time:\n",
        "                          relevant_alerts.append(alert)\n",
        "                # alert with no explicit start time (uncommon)\n",
        "                elif not alert_effective and alert_expires:\n",
        "                     if alert_expires >= start_time:\n",
        "                           relevant_alerts.append(alert)\n",
        "                # alert with no effective or expires time\n",
        "                else:\n",
        "                     pass\n",
        "\n",
        "\n",
        "        if relevant_alerts:\n",
        "            print(\"  Active Alerts:\")\n",
        "            for rel_alert in relevant_alerts:\n",
        "                print(f\"    - Event: {rel_alert.get('event', 'N/A')}\")\n",
        "                print(f\"      Headline: {rel_alert.get('headline', 'N/A')}\")\n",
        "\n",
        "    # save 7-day forecast data to JSON file\n",
        "    forecast_output_filename = \"noaa_7day_forecast.json\"\n",
        "    try:\n",
        "        with open(forecast_output_filename, 'w') as f:\n",
        "            json.dump(forecast_periods, f, indent=4)\n",
        "        print(f\"\\n7-day forecast data saved to {forecast_output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving 7-day forecast data to JSON: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Forecast data not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3a09464"
      },
      "source": [
        "#### Alerts Container\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5890883a"
      },
      "outputs": [],
      "source": [
        "# active weather alerts container\n",
        "\n",
        "# iterate through the parsed alerts\n",
        "if parsed_alerts:\n",
        "    print(f\"--- Active Weather Alerts for {user_location} ---\")\n",
        "    for alert in parsed_alerts:\n",
        "        print(f\"\\nEvent: {alert.get('event', 'N/A')}\")\n",
        "        print(f\"Headline: {alert.get('headline', 'N/A')}\")\n",
        "        print(f\"Description: {alert.get('description', 'N/A')}\")\n",
        "        print(f\"Effective: {alert.get('effective', 'N/A')}\")\n",
        "        print(f\"Expires: {alert.get('expires', 'N/A')}\")\n",
        "        print(\"-\" * 20) # separator for clarity\n",
        "else:\n",
        "    print(f\"No active weather alerts found for {user_location}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1477fe6"
      },
      "source": [
        "#### Air Quality Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "191da56c"
      },
      "outputs": [],
      "source": [
        "# AQ (air quality) graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "if aq_data:\n",
        "    # convert list of dictionaries to a pandas DataFrame for easier handling\n",
        "    aq_df = pd.DataFrame(aq_data)\n",
        "\n",
        "    if not aq_df.empty:\n",
        "      # extract date from the first entry (assuming all entries have same date)\n",
        "      observation_date = aq_df['DateObserved'].iloc[0] if 'DateObserved' in aq_df.columns else \"Unknown Date\"\n",
        "\n",
        "      # create a bar chart for AQI by Parameter\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.bar(aq_df['ParameterName'], aq_df['AQI'], color=['skyblue', 'lightgreen', 'salmon'])\n",
        "      plt.xlabel(\"Parameter\")\n",
        "      plt.ylabel(\"AQI (Air Quality Index)\")\n",
        "      # add the date to the title\n",
        "      plt.title(f\"Air Quality Index by Parameter on {observation_date}\")\n",
        "      plt.ylim(0, aq_df['AQI'].max() * 1.2) # set y-axis limit a bit above the max AQI\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "    else:\n",
        "      print(\"Air quality data is empty, cannot create visualizations.\")\n",
        "else:\n",
        "  print(\"Air quality data not available for visualization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ_PZI6KhNdG"
      },
      "source": [
        "#### Air Quality Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ3z4nkQg4WY"
      },
      "outputs": [],
      "source": [
        "# air quality container\n",
        "\n",
        "if aq_data:\n",
        "    print(\"--- Air Quality Data ---\")\n",
        "    for entry in aq_data:\n",
        "        print(f\"\\nDate Observed: {entry.get('DateObserved', 'N/A')}\")\n",
        "        print(f\"Hour Observed: {entry.get('HourObserved', 'N/A')} {entry.get('LocalTimeZone', 'N/A')}\")\n",
        "        print(f\"Reporting Area: {entry.get('ReportingArea', 'N/A')}\")\n",
        "        print(f\"Parameter Name: {entry.get('ParameterName', 'N/A')}\")\n",
        "        print(f\"AQI: {entry.get('AQI', 'N/A')}\")\n",
        "        # access the Name from the Category dictionary\n",
        "        category_name = entry.get('Category', {}).get('Name', 'N/A')\n",
        "        print(f\"Category: {category_name}\")\n",
        "        print(\"-\" * 20) # separator for clarity\n",
        "else:\n",
        "    print(\"Air quality data not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IimpN9Jx_w7Z"
      },
      "outputs": [],
      "source": [
        "# source: https://opendap.co-ops.nos.noaa.gov/axis/\n",
        "# different SOAP (Simple Object Access Protocol) web services that contains sample requests, sample responses and sample Java Client code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HET2ZhmXmKt"
      },
      "outputs": [],
      "source": [
        "# metadata sources:\n",
        "# https://opendap.co-ops.nos.noaa.gov/\n",
        "# https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/\n",
        "# This is CO-OPS' implementation of NOAA's (IOOS) Sensor Observational Service (SOS). CO-OPS SOS web services provide data retrieval of the latest observations and time series, for a \"collection\" of stations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmH8zwIfcNNQ"
      },
      "outputs": [],
      "source": [
        "# Access water level predictions from CO-OPS Sensor Observation Service (SOS)\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# CO-OPS Sensor Observation Service (SOS) URL\n",
        "sos_url = \"https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS\"\n",
        "\n",
        "# Base URL with service, request, version, and observedProperty\n",
        "# The observedProperty 'sea_surface_height_amplitude_due_to_equilibrium_ocean_tide' is from the provided Java snippet\n",
        "base_url = f\"{sos_url}?service=SOS&request=GetObservation&version=1.0.0&observedProperty=sea_surface_height_amplitude_due_to_equilibrium_ocean_tide\"\n",
        "\n",
        "# Parameters based on the provided Java snippet\n",
        "# NOTE: stationsIds and dates are hardcoded here as examples from the Java code.\n",
        "# You might want to make these dynamic based on user input or other data in your notebook.\n",
        "stations_ids = [\"8454000\", \"8454049\"]\n",
        "begin_date = \"2012-06-01\"\n",
        "end_date = \"2012-08-20\"\n",
        "datum = \"MSL\"  # Datum options: MHHW, MHW, MLLW, MLW, MSL, MTL, NAVD, STND, also can be null\n",
        "unit = \"Feet\" # Data unit options: Meters, Feet, also can be null\n",
        "response_format = \"csv\" # Response format options: csv, tsv, xml, kml\n",
        "\n",
        "# Construct the full API URL\n",
        "# Note: The SOS API uses different parameter names for station, begin_date, end_date compared to the datagetter API\n",
        "# Based on typical SOS GetObservation requests, these are often 'station', 'eventTime', etc.\n",
        "# However, the provided Java snippet uses dates as part of the query string, and stationsIds as an array.\n",
        "# A common way to handle multiple stations in SOS is to include them in the 'offering' or make multiple requests.\n",
        "# Let's try constructing a URL that includes station IDs and dates in a common SOS query format.\n",
        "# A more standard SOS query for a time range and multiple stations might look like this:\n",
        "# observedProperty=water_surface_height_above_reference_datum&offering=urn:ioos:station:NOAA.NOS.CO-OPS:8454000,urn:ioos:station:NOAA.NOS.CO-OPS:8454049&eventTime=2012-06-01T00:00:00Z/2012-08-20T23:59:59Z&responseFormat=text/csv\n",
        "\n",
        "# Let's adapt the URL construction based on a more standard SOS GetObservation request for a time series.\n",
        "# We'll use 'water_surface_height_above_reference_datum' as a common observed property for water level.\n",
        "# The 'offering' parameter is typically used to specify stations and networks.\n",
        "# The 'eventTime' parameter specifies the time range in ISO 8601 format.\n",
        "\n",
        "# Construct the offering string for multiple stations\n",
        "offering_string = \",\".join([f\"urn:ioos:station:NOAA.NOS.CO-OPS:{sid}\" for sid in stations_ids])\n",
        "\n",
        "# Construct the eventTime string in ISO 8601 format (assuming start of begin_date and end of end_date)\n",
        "# The SOS standard often requires timezone information, using 'Z' for UTC is common.\n",
        "event_time_string = f\"{begin_date}T00:00:00Z/{end_date}T23:59:59Z\"\n",
        "\n",
        "# Construct the full API URL with standard SOS parameters\n",
        "# Including datum and unit might require additional parameters or be part of the response format\n",
        "# Let's build the URL with the core parameters and the requested response format.\n",
        "api_url = (\n",
        "    f\"{sos_url}?service=SOS&request=GetObservation&version=1.0.0\"\n",
        "    f\"&observedProperty=water_surface_height_above_reference_datum\" # Using a common water level property\n",
        "    f\"&offering={offering_string}\"\n",
        "    f\"&eventTime={event_time_string}\"\n",
        "    f\"&responseFormat=text/{response_format}\"\n",
        ")\n",
        "\n",
        "# Add datum and unit if specified (these might not be standard SOS parameters this way,\n",
        "# but we'll include them if the API supports them)\n",
        "if datum:\n",
        "    api_url += f\"&datum={datum}\"\n",
        "if unit:\n",
        "    api_url += f\"&unit={unit}\"\n",
        "\n",
        "\n",
        "print(f\"Fetching data from: {api_url}\")\n",
        "\n",
        "# Make the HTTP GET request\n",
        "try:\n",
        "    response = requests.get(api_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Data fetched successfully.\")\n",
        "\n",
        "        # Read the content into a pandas DataFrame if the format is CSV\n",
        "        if response_format.lower() == 'csv':\n",
        "            try:\n",
        "                # Use StringIO to treat the text content as a file\n",
        "                sos_data_df = pd.read_csv(StringIO(response.text))\n",
        "                print(\"Data parsed into DataFrame.\")\n",
        "                # Display the first few rows of the DataFrame\n",
        "                display(sos_data_df.head())\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing CSV data: {e}\")\n",
        "                sos_data_df = None\n",
        "        else:\n",
        "            # For other formats, just print the text content\n",
        "            print(\"Fetched data (non-CSV format):\")\n",
        "            print(response.text[:500]) # Print first 500 characters to avoid flooding output\n",
        "            sos_data_df = None # DataFrame is not created for non-CSV formats\n",
        "\n",
        "    else:\n",
        "        print(f\"Error fetching data: Status code {response.status_code}\")\n",
        "        print(f\"Response content:\\n{response.text}\") # Print response content for debugging\n",
        "        sos_data_df = None # DataFrame is not created if fetch fails\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during data request: {e}\")\n",
        "    sos_data_df = None # DataFrame is not created if request fails\n",
        "\n",
        "# The fetched data (if CSV) is in sos_data_df DataFrame\n",
        "# If not CSV, the raw text is printed and sos_data_df is None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed168298"
      },
      "outputs": [],
      "source": [
        "# Access water level observations from CO-OPS Sensor Observation Service (SOS)\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# CO-OPS Sensor Observation Service (SOS) URL\n",
        "sos_url = \"https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS\"\n",
        "\n",
        "# Parameters based on the provided information\n",
        "# The observedProperty is water_surface_height_above_reference_datum\n",
        "observed_property = \"water_surface_height_above_reference_datum\"\n",
        "\n",
        "# Station Ids\n",
        "stations_ids = [\"1611400\", \"8311062\", \"8454049\", \"9034052\", \"9044020\"]\n",
        "\n",
        "# Begin date and end date for requested data\n",
        "begin_date = \"2011-11-30\"\n",
        "end_date = \"2012-01-10\"\n",
        "\n",
        "# Datum options: IGLD, MHHW, MHW, MLLW, MLW, MSL, MTL, NAVD, STND, also can be null\n",
        "datum = \"\" # Empty string for null\n",
        "\n",
        "# Response format options: csv, tsv, xml, kml\n",
        "response_format = \"csv\"\n",
        "\n",
        "# Construct the offering string for multiple stations\n",
        "# SOS typically uses URNs for stations in the 'offering' parameter\n",
        "offering_string = \",\".join([f\"urn:ioos:station:NOAA.NOS.CO-OPS:{sid}\" for sid in stations_ids])\n",
        "\n",
        "# Construct the eventTime string in ISO 8601 format (assuming start of begin_date and end of end_date)\n",
        "# The SOS standard often requires timezone information, using 'Z' for UTC is common.\n",
        "event_time_string = f\"{begin_date}T00:00:00Z/{end_date}T23:59:59Z\"\n",
        "\n",
        "# Construct the full API URL with standard SOS parameters\n",
        "api_url = (\n",
        "    f\"{sos_url}?service=SOS&request=GetObservation&version=1.0.0\"\n",
        "    f\"&observedProperty={observed_property}\"\n",
        "    f\"&offering={offering_string}\"\n",
        "    f\"&eventTime={event_time_string}\"\n",
        "    f\"&responseFormat=text/{response_format}\"\n",
        ")\n",
        "\n",
        "# Add datum if specified and not null\n",
        "if datum:\n",
        "    api_url += f\"&datum={datum}\"\n",
        "\n",
        "print(f\"Fetching water level observations from: {api_url}\")\n",
        "\n",
        "# Make the HTTP GET request\n",
        "try:\n",
        "    response = requests.get(api_url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Data fetched successfully.\")\n",
        "\n",
        "        # Read the content into a pandas DataFrame if the format is CSV\n",
        "        if response_format.lower() == 'csv':\n",
        "            try:\n",
        "                # Use StringIO to treat the text content as a file\n",
        "                # The CSV might have metadata lines at the beginning, need to handle that.\n",
        "                # Let's try reading directly first, and if that fails, inspect the content.\n",
        "                sos_obs_data_df = pd.read_csv(StringIO(response.text), comment='#') # Use comment to skip lines starting with #\n",
        "                print(\"Water level observations parsed into DataFrame.\")\n",
        "                # Display the first few rows of the DataFrame\n",
        "                display(sos_obs_data_df.head())\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing CSV data: {e}\")\n",
        "                print(\"Response content head:\")\n",
        "                print(response.text[:500]) # Print head of content for debugging\n",
        "                sos_obs_data_df = None # DataFrame is not created if parsing fails\n",
        "        else:\n",
        "            # For other formats, just print the text content\n",
        "            print(\"Fetched data (non-CSV format):\")\n",
        "            print(response.text[:500]) # Print first 500 characters to avoid flooding output\n",
        "            sos_obs_data_df = None # DataFrame is not created for non-CSV formats\n",
        "\n",
        "    else:\n",
        "        print(f\"Error fetching data: Status code {response.status_code}\")\n",
        "        print(f\"Response content:\\n{response.text}\") # Print response content for debugging\n",
        "        sos_obs_data_df = None # DataFrame is not created if fetch fails\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during data request: {e}\")\n",
        "    sos_obs_data_df = None # DataFrame is not created if request fails\n",
        "\n",
        "# The fetched data (if CSV) is in sos_obs_data_df DataFrame\n",
        "# If not CSV or parsing fails, sos_obs_data_df is None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "190b4c77"
      },
      "outputs": [],
      "source": [
        "# Access Meteorological Observations from CO-OPS Sensor Observation Service (SOS)\n",
        "# access to multiple years of meteorological & ancillary data from multiple stations\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CO-OPS Sensor Observation Service (SOS) URL\n",
        "sos_url = \"https://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS\"\n",
        "\n",
        "# Parameters based on the provided information\n",
        "# Observed properties\n",
        "observed_properties = [\"air_temperature\", \"winds\", \"rain_fall\"]\n",
        "\n",
        "# Station Ids\n",
        "stations_ids = [\"8454049\", \"8637689\", \"9754228\", \"9759394\"]\n",
        "\n",
        "# Begin date and end date for requested data\n",
        "begin_date_str = \"2011-12-01\"\n",
        "end_date_str = \"2012-03-21\"\n",
        "\n",
        "# Response format options: csv, tsv, xml, kml\n",
        "response_format = \"csv\"\n",
        "\n",
        "# Dictionary to store DataFrames for each observed property and station combination\n",
        "met_data_dfs = {}\n",
        "\n",
        "# Construct the eventTime string in ISO 8601 format\n",
        "# The SOS standard often requires timezone information, using 'Z' for UTC is common.\n",
        "event_time_string = f\"{begin_date_str}T00:00:00Z/{end_date_str}T23:59:59Z\"\n",
        "\n",
        "# Iterate through observed properties and stations to fetch data\n",
        "for prop in observed_properties:\n",
        "    for station_id in stations_ids:\n",
        "        # Construct the offering string for the current station\n",
        "        offering_string = f\"urn:ioos:station:NOAA.NOS.CO-OPS:{station_id}\"\n",
        "\n",
        "        # Construct the full API URL\n",
        "        api_url = (\n",
        "            f\"{sos_url}?service=SOS&request=GetObservation&version=1.0.0\"\n",
        "            f\"&observedProperty={prop}\"\n",
        "            f\"&offering={offering_string}\"\n",
        "            f\"&eventTime={event_time_string}\"\n",
        "            f\"&responseFormat=text/{response_format}\"\n",
        "        )\n",
        "\n",
        "        print(f\"\\nFetching {prop} data for station {station_id} from: {api_url}\")\n",
        "\n",
        "        # Make the HTTP GET request\n",
        "        try:\n",
        "            response = requests.get(api_url)\n",
        "\n",
        "            # Check if the request was successful (status code 200)\n",
        "            if response.status_code == 200:\n",
        "                print(\"Data fetched successfully.\")\n",
        "\n",
        "                # Read the content into a pandas DataFrame if the format is CSV\n",
        "                if response_format.lower() == 'csv':\n",
        "                    try:\n",
        "                        # Use StringIO to treat the text content as a file\n",
        "                        # Use comment to skip lines starting with #\n",
        "                        df = pd.read_csv(StringIO(response.text), comment='#')\n",
        "\n",
        "                        # Store the DataFrame in the dictionary using a composite key\n",
        "                        met_data_dfs[f\"{prop}_{station_id}\"] = df\n",
        "                        print(f\"{prop} data for station {station_id} parsed into DataFrame.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error parsing CSV data for {prop} at station {station_id}: {e}\")\n",
        "                        print(\"Response content head:\")\n",
        "                        print(response.text[:500]) # Print head of content for debugging\n",
        "\n",
        "                else:\n",
        "                    # For other formats, just print the text content and indicate no DataFrame\n",
        "                    print(f\"Fetched data for {prop} at station {station_id} (non-CSV format):\")\n",
        "                    print(response.text[:500]) # Print first 500 characters\n",
        "            else:\n",
        "                print(f\"Error fetching data for {prop} at station {station_id}: Status code {response.status_code}\")\n",
        "                print(f\"Response content:\\n{response.text}\") # Print response content for debugging\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error during data request for {prop} at station {station_id}: {e}\")\n",
        "\n",
        "# After the loop, met_data_dfs dictionary contains DataFrames for successful fetches\n",
        "# You can now access and process the DataFrames from met_data_dfs\n",
        "if met_data_dfs:\n",
        "    print(\"\\n--- Summary of Fetched Meteorological Data ---\")\n",
        "    for key, df in met_data_dfs.items():\n",
        "        print(f\"DataFrame for {key}: {len(df)} rows\")\n",
        "        # Optional: Display head of each DataFrame\n",
        "        # print(df.head())\n",
        "else:\n",
        "    print(\"\\nNo meteorological data could be fetched for the specified parameters and stations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fd1c9a1"
      },
      "outputs": [],
      "source": [
        "# Define target observed properties and fetch data from the nearest CO-OPS station\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Ensure nearest_coops_station is available\n",
        "if 'nearest_coops_station' not in globals() or not nearest_coops_station or 'id' not in nearest_coops_station:\n",
        "    print(\"Nearest CO-OPS station information is not available. Cannot fetch data for observed properties.\")\n",
        "else:\n",
        "    station_id_num = nearest_coops_station['id']\n",
        "    print(f\"Fetching data for observed properties from nearest station: {nearest_coops_station.get('name', station_id_num)} (ID: {station_id_num})\")\n",
        "\n",
        "    # Define the list of target observed properties\n",
        "    observed_properties = [\n",
        "        'air_temperature',\n",
        "        'air_pressure',\n",
        "        'water_electrical_conductivity',\n",
        "        'sea_water_speed_and_direction', # Grouped wind speed and direction\n",
        "        'sea_water_salinity',\n",
        "        'water_surface_height',\n",
        "        'sea_water_temperature',\n",
        "        'wind_speed_and_direction', # Grouped wind speed and direction\n",
        "        'relative_humidity',\n",
        "        'rain_fall',\n",
        "        'visibility'\n",
        "    ]\n",
        "\n",
        "    # Dictionary to store fetched data for each property\n",
        "    coops_observed_data = {}\n",
        "\n",
        "    # Define a suitable recent date range (e.g., last 24 hours)\n",
        "    end_time = datetime.utcnow()\n",
        "    begin_time = end_time - timedelta(days=1) # Fetch data for the last 24 hours\n",
        "    # Format the date range in ISO 8601 format with Z for UTC\n",
        "    event_time_string = f'{begin_time.isoformat()}Z/{end_time.isoformat()}Z'\n",
        "\n",
        "\n",
        "    # Iterate through observed properties and fetch data\n",
        "    for prop in observed_properties:\n",
        "        print(f\"\\nAttempting to fetch data for: {prop}\")\n",
        "\n",
        "        # Base parameters for the SOS GetObservation request\n",
        "        params = {\n",
        "            'service': 'SOS',\n",
        "            'request': 'GetObservation',\n",
        "            'version': '1.0.0',\n",
        "            'observedProperty': prop,\n",
        "            'offering': f'urn:ioos:station:NOAA.NOS.CO-OPS:{station_id_num}', # Use station ID in offering\n",
        "            'eventTime': event_time_string,\n",
        "            'responseFormat': 'text/csv', # Request CSV format\n",
        "        }\n",
        "\n",
        "        # Adjust parameters based on property or known endpoints/networks\n",
        "        if prop == 'water_surface_height':\n",
        "             params['observedProperty'] = 'water_surface_height_above_reference_datum'\n",
        "             # params['datum'] = 'MLLW' # Optional datum\n",
        "             # params['unit'] = 'english' # Optional unit\n",
        "        elif prop == 'sea_water_speed_and_direction':\n",
        "             params['observedProperty'] = 'sea_water_speed,direction_of_sea_water_velocity' # Attempt multiple properties\n",
        "             params['offering'] = f'urn:ioos:network:NOAA.NOS.CO-OPS:CurrentsActive,urn:ioos:station:NOAA.NOS.CO-OPS:{station_id_num}' # Use CurrentsActive network\n",
        "             # params['unit'] = 'm/s' # Optional unit\n",
        "        elif prop == 'wind_speed_and_direction':\n",
        "             params['observedProperty'] = 'wind_speed,wind_from_direction,wind_speed_of_gust' # Attempt multiple properties\n",
        "             params['offering'] = f'urn:ioos:network:NOAA.NOS.CO-OPS:MetActive,urn:ioos:station:NOAA.NOS.CO-OPS:{station_id_num}' # Use MetActive network\n",
        "             # params['unit'] = 'm/s' # Optional unit\n",
        "        elif prop in ['air_temperature', 'air_pressure', 'water_electrical_conductivity',\n",
        "                      'sea_water_salinity', 'sea_water_temperature', 'relative_humidity',\n",
        "                      'rain_fall', 'visibility']:\n",
        "             # These properties are typically under the MetActive network\n",
        "             params['offering'] = f'urn:ioos:network:NOAA.NOS.CO-OPS:MetActive,urn:ioos:station:NOAA.NOS.CO-OPS:{station_id_num}'\n",
        "             # Handle potential unit specifications if needed (e.g., Fahrenheit for temperature)\n",
        "             if prop in ['air_temperature', 'sea_water_temperature']:\n",
        "                 pass # Add unit parameter if needed: params['unit'] = 'Fahrenheit'\n",
        "\n",
        "\n",
        "        # Construct the full API URL from base URL and parameters\n",
        "        # Correcting the URL construction: use requests.get(url, params=params)\n",
        "        # instead of manually building the query string to avoid issues.\n",
        "        api_url = sos_url # Use the base SOS URL\n",
        "\n",
        "\n",
        "        print(f\"  Requesting data for {prop} from: {api_url} with params: {params}\") # Print URL and params for debugging\n",
        "\n",
        "        # Make the HTTP GET request\n",
        "        try:\n",
        "            # Use the params argument to let requests build the query string correctly\n",
        "            response = requests.get(api_url, params=params)\n",
        "\n",
        "            # Check if the request was successful (status code 200)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"  Data fetched successfully for {prop}.\")\n",
        "\n",
        "                # Process the response based on format (assuming CSV for now)\n",
        "                if params.get('responseFormat', '').lower() == 'text/csv':\n",
        "                    try:\n",
        "                        # Use StringIO to treat the text content as a file\n",
        "                        # Use comment to skip metadata lines starting with #\n",
        "                        df = pd.read_csv(StringIO(response.text), comment='#')\n",
        "\n",
        "                        # Check if the DataFrame is empty or contains an error message\n",
        "                        if not df.empty and 'ExceptionReport' not in response.text:\n",
        "                           coops_observed_data[prop] = df # Store DataFrame\n",
        "                           print(f\"  {prop} data parsed into DataFrame ({len(df)} rows).\")\n",
        "                        else:\n",
        "                           print(f\"  No data or error response received for {prop}. Response head:\")\n",
        "                           print(response.text[:200]) # Print head of content for debugging\n",
        "                           coops_observed_data[prop] = f\"No data/Error: {response.text[:200]}\" # Store error info\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Error parsing CSV data for {prop}: {e}\")\n",
        "                        coops_observed_data[prop] = response.text # Store raw text if parsing fails\n",
        "                        print(\"  Response content head:\")\n",
        "                        print(response.text[:200]) # Print head of content for debugging\n",
        "                else:\n",
        "                    # Store raw text for non-CSV formats\n",
        "                    coops_observed_data[prop] = response.text\n",
        "                    print(f\"  Fetched raw data for {prop} (non-CSV format).\")\n",
        "\n",
        "            else:\n",
        "                print(f\"  Error fetching data for {prop}: Status code {response.status_code}\")\n",
        "                coops_observed_data[prop] = f\"Error: Status code {response.status_code}, Content: {response.text[:200]}\" # Store error info\n",
        "                print(f\"  Response content head:\\n{response.text[:200]}\") # Print head of content for debugging\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  Error during data request for {prop}: {e}\")\n",
        "            coops_observed_data[prop] = f\"Error: Request Exception - {e}\" # Store error info\n",
        "\n",
        "    # Display summary of fetched data\n",
        "    print(\"\\n--- Summary of Fetched CO-OPS Observed Data from Nearest Station ---\")\n",
        "    if coops_observed_data:\n",
        "        for prop, data in coops_observed_data.items():\n",
        "            if isinstance(data, pd.DataFrame):\n",
        "                print(f\"  {prop}: Successfully fetched DataFrame ({len(data)} rows)\")\n",
        "                # Optional: print head of DataFrame\n",
        "                # display(data.head())\n",
        "            elif isinstance(data, str) and data.startswith(\"Error:\"):\n",
        "                 print(f\"  {prop}: {data}\")\n",
        "            elif isinstance(data, str) and data.startswith(\"No data/Error:\"):\n",
        "                 print(f\"  {prop}: {data}\")\n",
        "            else:\n",
        "                print(f\"  {prop}: Successfully fetched raw data (type: {type(data)})\")\n",
        "    else:\n",
        "        print(\"  No data could be fetched for any of the observed properties.\")\n",
        "\n",
        "# The fetched and processed data is stored in the coops_observed_data dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQvWNm4NmJNo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a812e619"
      },
      "outputs": [],
      "source": [
        "from geopy.distance import geodesic\n",
        "\n",
        "# find nearest stations\n",
        "if latitude is None or longitude is None:\n",
        "    print(\"User location (latitude or longitude) not available. Skipping distance calculation.\")\n",
        "elif 'coops_stations_data' not in globals() or not coops_stations_data:\n",
        "    print(\"CO-OPS stations data not available. Skipping distance calculation.\")\n",
        "else:\n",
        "    # iterate through coops_stations_data list\n",
        "    for station in coops_stations_data:\n",
        "        # extract latitude and longitude for each station\n",
        "        station_lat = station.get('latitude')\n",
        "        station_lon = station.get('longitude')\n",
        "\n",
        "        # calculate distance\n",
        "        if station_lat is not None and station_lon is not None:\n",
        "            try:\n",
        "                user_coords = (latitude, longitude)\n",
        "                station_coords = (station_lat, station_lon)\n",
        "                distance = geodesic(user_coords, station_coords).miles\n",
        "            except ValueError:\n",
        "                # handle potential value error during calculation\n",
        "                distance = float('inf')\n",
        "        else:\n",
        "            distance = float('inf') # assign infinity if coords are missing\n",
        "\n",
        "        # add a 'distance' key to each station's dictionary\n",
        "        station['distance'] = distance\n",
        "    print(\"Distance to each CO-OPS station calculated.\")\n",
        "\n",
        "# find station with shortest distance to user\n",
        "if 'coops_stations_data' not in globals() or not coops_stations_data:\n",
        "    print(\"CO-OPS station data not available. Cannot find the nearest station.\")\n",
        "else:\n",
        "    try:\n",
        "        nearest_coops_station = min(coops_stations_data, key=lambda station: station.get('distance', float('inf')))\n",
        "\n",
        "        # print the ID, name, and distance of the nearest station\n",
        "        station_id = nearest_coops_station.get('id', 'N/A')\n",
        "        station_name = nearest_coops_station.get('name', 'N/A')\n",
        "        station_distance = nearest_coops_station.get('distance', float('inf'))\n",
        "\n",
        "        if station_distance != float('inf'):\n",
        "            print(f\"\\n------- Nearest CO-OPS station ------- \\nStation ID: {station_id}, name: {station_name}, distance: {station_distance:.2f} miles\")\n",
        "        else:\n",
        "            print(\"Nearest CO-OPS station found, but distance information is missing.\")\n",
        "    except ValueError:\n",
        "        print(\"Could not find the nearest CO-OPS station.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1a02df4"
      },
      "source": [
        "import json\n",
        "\n",
        "# Display the daily maximum and minimum water level data\n",
        "if 'daily_max_min_water_level_data' in globals() and daily_max_min_water_level_data:\n",
        "    print(\"\\n--- Daily Maximum/Minimum Water Level Data ---\")\n",
        "    if 'data' in daily_max_min_water_level_data and daily_max_min_water_level_data['data']:\n",
        "        for entry in daily_max_min_water_level_data['data']:\n",
        "            if 'dailyMaxHourly' in entry and entry['dailyMaxHourly']:\n",
        "                 print(\"Daily Max:\")\n",
        "                 for max_entry in entry['dailyMaxHourly']:\n",
        "                     print(f\"  Date: {max_entry.get('dateHourly')}, Time: {max_entry.get('timeHourly')}, Value: {max_entry.get('value')}\")\n",
        "            if 'dailyMinHourly' in entry and entry['dailyMinHourly']:\n",
        "                 print(\"Daily Min:\")\n",
        "                 for min_entry in entry['dailyMinHourly']:\n",
        "                     print(f\"  Date: {min_entry.get('dateHourly')}, Time: {min_entry.get('timeHourly')}, Value: {min_entry.get('value')}\")\n",
        "    else:\n",
        "        print(\"No daily maximum/minimum water level data available in the response.\")\n",
        "\n",
        "    # Save the daily max/min water level data to a JSON file\n",
        "    output_filename = \"daily_max_min_water_level_data.json\"\n",
        "    try:\n",
        "        with open(output_filename, 'w') as f:\n",
        "            json.dump(daily_max_min_water_level_data, f, indent=4)\n",
        "        print(f\"\\nDaily max/min water level data saved to {output_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving daily max/min water level data to JSON: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Daily max/min water level data is not available to display or save.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fcca082"
      },
      "source": [
        "from geopy.distance import geodesic\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure user location is available\n",
        "if 'latitude' not in globals() or latitude is None or 'longitude' not in globals() or longitude is None:\n",
        "    print(\"User location (latitude or longitude) not available. Cannot filter stations by proximity.\")\n",
        "elif 'formatted_stations_df' not in globals() or formatted_stations_df.empty:\n",
        "    print(\"Formatted stations data is not available. Cannot filter stations.\")\n",
        "else:\n",
        "    user_coords = (latitude, longitude)\n",
        "    proximity_miles = 80\n",
        "\n",
        "    # Calculate the distance from the user location to each station in miles\n",
        "    # Ensure Latitude and Longitude columns are numeric\n",
        "    formatted_stations_df['Latitude'] = pd.to_numeric(formatted_stations_df['Latitude'], errors='coerce')\n",
        "    formatted_stations_df['Longitude'] = pd.to_numeric(formatted_stations_df['Longitude'], errors='coerce')\n",
        "\n",
        "    # Drop rows where Lat/Lon could not be converted to numeric\n",
        "    stations_for_distance_calc = formatted_stations_df.dropna(subset=['Latitude', 'Longitude']).copy()\n",
        "\n",
        "\n",
        "    def calculate_distance_miles(row):\n",
        "        station_coords = (row['Latitude'], row['Longitude'])\n",
        "        try:\n",
        "            return geodesic(user_coords, station_coords).miles\n",
        "        except ValueError:\n",
        "            # Handle cases where coordinates might be invalid\n",
        "            return float('inf')\n",
        "\n",
        "\n",
        "    stations_for_distance_calc['Distance (miles)'] = stations_for_distance_calc.apply(calculate_distance_miles, axis=1)\n",
        "\n",
        "    # Filter for stations within the specified proximity\n",
        "    merged_nearby_stations_df = stations_for_distance_calc[stations_for_distance_calc['Distance (miles)'] <= proximity_miles].sort_values(by='Distance (miles)')\n",
        "\n",
        "    print(f\"\\n--- Stations within {proximity_miles} miles of your location ---\")\n",
        "    if not merged_nearby_stations_df.empty:\n",
        "        display(merged_nearby_stations_df.to_string(index=False)) # Display all rows without index\n",
        "        # Optionally, you could save this filtered list to a file\n",
        "        # merged_nearby_stations_df.to_csv(\"nearby_coops_stations.csv\", index=False)\n",
        "        # print(f\"\\nNearby stations saved to nearby_coops_stations.csv\")\n",
        "    else:\n",
        "        print(\"No stations found within the specified proximity.\")\n",
        "\n",
        "# The filtered DataFrame is available as merged_nearby_stations_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522e75e8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "if 'tide_predictions_result' in globals() and tide_predictions_result and 'predictions' in tide_predictions_result:\n",
        "    predictions_list = tide_predictions_result['predictions']\n",
        "\n",
        "    if predictions_list:\n",
        "        print(\"--- Tide Predictions ---\")\n",
        "        # Create a pandas DataFrame from the predictions list\n",
        "        tide_predictions_df = pd.DataFrame(predictions_list)\n",
        "\n",
        "        # Display the DataFrame as a table\n",
        "        # Using to_string to display the full table without truncation\n",
        "        print(tide_predictions_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"No tide predictions available in the data.\")\n",
        "else:\n",
        "    print(\"Tide predictions data not available.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dc07cac",
        "outputId": "def6b192-53ed-4a1e-df17-8da3dd616fbd"
      },
      "source": [
        "import json\n",
        "\n",
        "try:\n",
        "    # Define the filename\n",
        "    filename = \"user_data_noaa_coops_tide.json\"\n",
        "\n",
        "    # Load the JSON data from the file\n",
        "    with open(filename, 'r') as f:\n",
        "        tide_data = json.load(f)\n",
        "\n",
        "    # Access the 'about' string using get() for safe access\n",
        "    about_text = tide_data.get('tide_predictions', {}).get('background', {}).get('about', 'About text not found in JSON.')\n",
        "\n",
        "    # Print the string. The print() function interprets the \\n characters as line breaks.\n",
        "    print(f\"--- About Tide Predictions (from {filename}) ---\")\n",
        "    print(about_text)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {filename} not found. Please ensure the file was saved correctly.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from {filename}. The file might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- About Tide Predictions (from user_data_noaa_coops_tide.json) ---\n",
            "The rising and falling of the sea, \"the tides,\" are a phenomenon upon which we can always depend. Caused by the gravitational pull of the moon and the sun, tides are very long-period waves that move through the ocean and progress toward the coastlines where they appear as the regular rise and fall of the sea surface. \n",
            "\n",
            "Tide predictions provide the times and heights for the astronomical tides. Predictions are based on the analysis of data collected at coastal locations maintained by the National Oceanic and Atmospheric Administration's (NOAA), Center for Operational Oceanographic Products and Services (CO-OPS). \n",
            "\n",
            "CO-OPS maintains the National Water Level Observation Network (NWLON), an observation network with more than 200 permanent water level stations on the coasts and Great Lakes. This system allows NOAA to provide the official tidal predictions for the nation. Accurate water level data is critical for safe and efficient marine navigation and for the protection of infrastructure along the coast. \n",
            "\n",
            "Learn more through https://tidesandcurrents.noaa.gov/water_level_info.html\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J3D_nNAcDU1i",
        "UXxnXNpLJ7YR",
        "b02f8ec9",
        "KMs9YB-Nh2o1",
        "fa58acc9",
        "3gaYomvD7Yb7",
        "PcHeXSXPlVfT",
        "PqQHIozNutOm",
        "6vy-uZ1oxag5",
        "_WBSjf3EzW_P",
        "m0KH5ZoR3CDA",
        "9b91637f"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}